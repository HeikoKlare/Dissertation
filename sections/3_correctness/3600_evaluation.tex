\chapter{Evaluation and Discussion 
    \pgsize{20 p.}
}
\label{chap:correctness_evaluation}

\todo{Discuss that the formalism is proven correct. We only valide the error classification and the prevention 
strategies.
Additional goal: Find how likely specific kinds of errors are to know how relevant the different error categories are.
}

We have shown that we cannot define an orchestration that is always correct and optimal. However, we presented techniques to optimize as far as possible.
We evaluate here, in how many cases this is sufficient in practice. To do so, we have the case study (Torsten / Timur) were we identified issues. We map them to our approaches and show that at least in this case study almost all problems would have been detected with them.


\section{Decomposition}
\begin{copiedFrom}{SoSym MPM4CPS}

We have conducted a case study to evaluate correctness and applicability of our approach.
The evaluation focuses on the appropriate operationalization of the formal approach, which is proven correct, and its practical applicability in terms of providing an appropriate level of conservativeness. This defines our contribution \ref{contrib:evaluation}.



\subsection{Goals}

\paragraph{Correctness}
Correctness of our approach means that it is able to classify a given set of consistency relations as compatible or otherwise does not reveal a result.
This especially means that it operates conservatively.
The formal approach presented in \autoref{chap:formal:approach} is proven correct by \autoref{theorem:redundancycompatibility}, \autoref{theorem:treecompatibility} and \autoref{corollary:transitiveredundancycompatibility}, such that we do not need to further evaluate its correctness.
For that reason, the correctness evaluation focuses on the operationalized approach presented in \autoref{chap:prevention:compatibility} and \autoref{chap:prevention:compatibility:redundancies}.
We investigate whether the operationalized approach reveals expected results, indicating that the mapping of our formalism to QVT-R is correct, and especially that it operates conservatively.

\paragraph{Applicability}
Since the approach defines a fully automated algorithm, which does not require further input apart from the QVT-R relations to check, applicability may only be restricted by inadequate outputs, which are correct but not useful for the user.
In consequence, we consider applicability of our approach especially in terms of the practicality regarding its degree of conservativeness.
If the approach is not able to identify compatibility in too many cases, in which relations are actually compatible, applicability would be limited.
For that reason, we aimed to identify in how many cases the approach is not able to prove compatibility although compatibility is given, and what the reasons for those results are.
It is of special interest whether those are conceptual issues of the formal approach or a limitation of the operationalization that may be fixed by other realization approaches.

% \begin{itemize}
%     \item Correctness: Focused on operationalized approach, since formal approach is proven correct: Show that algorithm reveals expected results, which indicates that the mapping of the formalism to QVT-R is correct and conservative as expected and that the implementation is correct.
%     \item Applicability: Show practical applicability of the overall approach, especially regarding the degree of conservativeness. Applicability is restricted if the approach does not reveal a result although the relation are compatible in too many cases.
%     We will also consider the reasons for those non-results, i.e. whether it is a conceptual issue of the formal approach or a limitation of the operationalization.
% \end{itemize}

\subsection{Methodology}
\label{sec:evaluation:methodology}

To empirically evaluate correctness and applicability of our approach, we developed a prototypical implementation and applied it to exemplary case studies.
We give an overview of that prototype in the subsequent subsection.
\autoref{tab:scenarios} summarizes the case studies to which we applied the approach.
Each of those scenarios consists of three or four metamodels and especially comprises primitive data types and operations.
They were specifically developed to evaluate our approach by defining as many kinds of relations that can be expressed with QVT-R as possible, thus also reflecting edge cases.

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.2}%
    %\rowcolors{2}{white}{gray!15}
    \setlength\tabcolsep{4 pt}
    \begin{tabular}{L{0.5cm} L{7.5cm} C{2cm}}
        \toprule
        \textbf{\#} & \textbf{Scenario Description} & \textbf{Compatible} \\
        \midrule
        1 & Three equal String attributes of three metamodels & \cmark\\
        2 & Six equal String attributes of three metamodels & \cmark\\
        3 & Concatenation of two String attributes & \cmark\\
        4 & Double concatenation of four String attributes & \cmark\\
        5 & Substring in a String attribute & \cmark\\
        6 & Substring in a String attribute with precondition & \cmark\\
        7 & Precondition with all primitive datatypes & \cmark\\
        8 & Absolute value of Integer attribute with precondition & \cmark\\ 
        9 & Transitive equality for three Integer attributes & \cmark\\
        10 & Inequalities for three Integer attributes & \cmark\\
        11 & Contradictory equalities for three Integer attributes & \xmark\\
        12 & Contradictory inequalities for three Integer attributes & \xmark\\
        13 & Constant property template items & \cmark\\
        14 & Linear equations with three Integer attributes & \cmark\\ 
        15 & Contradictory linear equations for three Int. attributes & \xmark\\
        16 & Emptiness of various OCL sequence and set literals & \xmark\\
        17 & Equal String attributes for four metamodels & \cmark\\
        18 & Transitive inclusions in sequences & \cmark\\
        19 & Comparison of role names in three metamodels & \cmark\\
        \bottomrule
    \end{tabular}
    \caption{Example scenarios of consistency relations and their compatibility property, from \cite{pepin2019ma}}
    \label{tab:scenarios}
\end{table}

%%
%% Description of scenarios, definition of ground truth
%%
We developed 14 compatible and four incompatible transformations, according to our \autoref{def:compatibility} for compatibility.
Thus, we know the ground truth regarding compatibility of transformations for each scenario by construction.
Applying our prototypical implementation to those scenarios classifies them as compatible (\emph{positives}) or makes no statement about compatibility (\emph{negatives}), i.e., they could either be compatible or not.
Considering which of the results are actually correct gives us insights on correctness and applicability.

%%
%% Correctness interpretation of metrics
%%
The approach is correct, which especially means that it operates conservatively, if it does not classify any transformation networks as compatible although they are not.
This means that no \emph{false positives} are allowed to occur or otherwise the approach would, per definition, be incorrect.
In other words, the \emph{precision} of the approach has to be 1:
\begin{align*}
    \formulaskip &
    Precision = \frac{\mathtext{true positives}}{\mathtext{true positives + false positives}}
\end{align*}
%If there are any false positives, the approach is, by definition, not correct.

%%
%% Applicability interpretation of metrics
%%
Applicability of the approach depends on the degree of conservativeness, i.e., in how many cases it does not identify a transformation network as compatible although it is.
This is reflected by the number of \emph{false negatives} and, when compared to the \emph{true positives} known as the \emph{recall}, gives insights on the degree of conservativeness:
\begin{align*}
    &
    Recall = \frac{\mathtext{true positives}}{\mathtext{true positives + false negatives}}
\end{align*}
A high recall value indicates high applicability of the approach in terms of not being too conservative.

%\begin{itemize}
    %\item Apply a prototypical implementation of the approach to exemplary case studies.
    %\item Scenarios are summarized in \autoref{tab:scenarios}, composed of 3 or 4 metamodels, primitive data types and operations.
    %\item Scenarios only reflect supported QVT-R and OCL constructs, for which an analysis is implemented yet.
    %\item Define ground truth manually, labeling the inputs with being compatible or not, according to manually checking the definition.
    %\item We consider positives as relation sets classified as compatible and negatives as those for which no statement about compatibility is made (they may be incompatible, but could also be compatible -- conservativeness)
    %\item Evaluate results regarding precision (must be 1 if approach is correct): $\frac{true positives}{true positives + false positives}$ -- if there are any false positives, the approach is not correct
    %\item Evaluate results regarding recall: $\frac{true positives}{true positives + false negatives}$ -- false negatives are which are not classified although they are compatible, i.e. they are those which are not identified as compatible due to the approach being conservative.
    %\item High recall indicates high applicability, as there are few scenarios in which compatibility is not identified.
%\end{itemize}


\subsection{Prototypical Implementation}
\label{sec:evaluation:prototype}

The decomposition procedure presented in Sections \ref{chap:prevention:compatibility} and~\ref{chap:prevention:compatibility:redundancies} resulted in the implementation of a prototype, which is available on GitHub~\cite{decompositionGithub}. The formal approach of this paper addresses a common problem in the development of cyber-physical systems: inconsistencies lead to the development of incompatible artifacts, which in turn can lead to unexpected behavior. Therefore, the practicality of our approach matters. A tool for proving compatibility could be easily integrated into the development process of a transformation network in order to assist developers and domain experts.

\subsubsection{Features}

The implementation of the procedure takes a set of \qvtr transformations as an input and outputs a list of redundant \qvtr relations. This list must be compared to the initial consistency specification. There are two scenarios: either the delivered specification %without redundant relations 
forms a consistency relation tree or there are still cycles left. % after the procedure. 
In the former case, compatibility is proven. In the latter case, remaining cycles require the developer's attention. This may be due to incompatibility or the inability of the procedure to prove redundancy.

In addition to the features of the procedure, the prototype provides an input validation. There are two reasons why a consistency specification may cause the procedure not to operate correctly. First, specifications can be composed of not well-formed transformations, i.e., \qvtr transformations that are syntactically incorrect. In this case, the specification is not usable and the procedure immediately exits. There is another scenario: specifications that are well-formed but not valid. For example, this occurs when two transformations have the same name or when a \qvtr domain pattern uses a nonexistent class. Although this scenario is non-blocking, i.e., the decomposition procedure still produces a result, the output must be interpreted with caution. To assist the developer, the procedure displays semantic errors in the specification at the beginning of the parsing. In the end, the procedure is intended to be non-intrusive, i.e., it is does not alter any artifact and can be used at any moment during the development process, i.e., by logging its results.

\subsubsection{Implementation}

Technical choices are mostly driven by the support of model-driven engineering technologies. One important initiative to this end is the \ac{MDA}~\cite{mda}. %, an approach of the Object Management Group. 
The decomposition procedure makes use of many specifications recommended by the MDA, including \qvtr for the definition of transformations, \ac{EMOF} for metamodels~\cite{mof} and OCL for constraints over metamodels~\cite{ocl}. Eclipse provides an implementation of these languages within the \ac{EMF}~\cite{steinberg2009emf}. As a consequence, metamodels of the decomposition procedure are implemented using \textit{Ecore}, a meta-metamodel that is compliant with \ac{EMOF}. EMF supports a number of model-to-model transformation languages through the \textit{Eclipse MMT} project. In particular, the \textit{QVT Declarative} (QVTd) component provides a parser for QVT-R transformations. As QVT-R relies on OCL, QVTd makes use of \textit{Eclipse OCL}, an implementation of the OCL language.

Regarding the strategy for redundancy testing, the implementation of the decomposition procedure requires the use of an SMT solver. Most SMT solvers are based on SMT-LIB, an initiative that provides a common input/output language for SMT instances. The prototype relies on the Z3 theorem prover, an SMT solver with a Java binding and a large number of theories supported~\cite{z32008}.

% \todo[inline, color=kit-orange100]{footnote/ref > github URL?}
%% Conservativeness, etc.

% \begin{itemize}
%     \item \textbf{Prototype}. We developed an implementation of the decomposition procedure using:
%         \begin{itemize}
%             \item QVTd, a partial implementation of QVT-R and QVT-C in Eclipse MMT
%             \item Z3, an automated theorem prover
%         \end{itemize}
% \end{itemize}


\subsection{Results}

In the following, we present the results of our evaluation regarding the methodology proposed in \autoref{sec:evaluation:methodology} applied to the prototypical implementation introduced in \autoref{sec:evaluation:prototype}.
The classification results are summarized in \autoref{tab:classificationresults}.

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.4}%
    \setlength\tabcolsep{4 pt}
    \begin{tabular}{L{2.2cm} C{3cm} C{2.3cm}}
        \toprule
         & \textbf{Classified Compatible} & \textbf{Unclassified} \\
         \midrule
         \textbf{Compatible} & 12 & 4\\
         \textbf{Incompatible} & 0 & 3\\
         \bottomrule
    \end{tabular}
    \caption{Number of scenarios from \autoref{tab:scenarios} regarding actual compatibility and their classification by our approach.}
    \label{tab:classificationresults}
\end{table}

\subsubsection{Correctness}
As discussed before, the correctness of our approach in terms of conservative behavior is proven for the formal approach by construction.
Since the operationalized approach is based on that formalization, correctness is also given by construction provided that the following requirements are fulfilled:
\begin{enumerate}
    \item All relevant QVT-R relations are considered, i.e., all QVT-R relations are represented in the property graph to be considered as consistency relations to be checked.
    \item Consistency rules in QVT-R are defined using variables, so all constructs referring to these variable have to be considered. This especially means that all  template expressions need to be considered for the property graph construction, namely property template items, preconditions and invariants.
\end{enumerate}

We ensured that all these relevant elements are considered by construction of the approach discussed in \autoref{chap:prevention:compatibility} and \autoref{chap:prevention:compatibility:redundancies}.
However, the results of the case study further validate that we did not miss any relevant parts of QVT-R relations.
In fact, the results summarized in \autoref{tab:classificationresults} show that we have a precision of 1, thus having no incompatible scenarios classified as compatible by error:

\begin{align*}
    &
    Precision = \frac{\mathtext{true positives}}{\mathtext{true positives + false positives}} = \frac{12}{12+0} = 1
\end{align*}


\subsubsection{Applicability}

Applicability of the presented approach depends on its degree of conservativeness.
If it is not able to prove compatibility of compatible transformation networks in too many cases, applicability is reduced.
In general, the conservative behavior results especially from two reasons:
\begin{enumerate}
    \item \autoref{def:leftequalredundancy} for left-equal redundancy, which is used to prove compatibility of a network, may be a too strong requirement for identifying compatibility-preserving consistency relations.
    \item Consistency relations as defined in \autoref{def:consistencyrelation} are extensional specifications and thus usually enumerate infinite sets of elements, which are impossible to compare programmatically.
    For that reason, our operationalized approach relies on intensional specifications, which describe how consistent pairs of elements can be derived.
    These specifications are written in OCL. 
    However, OCL in general is undecidable, because it can be transformed into first-order logic~\cite{beckert2002ocltranslation}.
\end{enumerate}
%%
%% Discussion of OCL analyzability
%%
Especially formulae that contain many quantifiers are hard to analyze.
For that reason, the number of variables used in a consistency relation is crucial, as these variables are translated to existentially quantified formulae.
Although not all available OCL constructs might be necessary to describe relevant consistency relations, constructs involving operations on sets and strings are problematic.
Operation collections are transferred to quantified formulae, which are hard to analyze.
Reasoning about strings is problematic, because some OCL operations like \texttt{toUpper} and \texttt{toLower} cannot be easily transferred to state-of-the-art SMT solvers like Z3 and thus cannot be considered for detecting redundancy.
Furthermore, SMT solvers use heuristics, so it not possible to formally evaluate which kinds of relations can be analyzed.

%\begin{itemize}
    %\item Relations are usually infinite sets of elements, which are impossible to compare programmatically. For that reason, the operationalized approach uses intensional specifications of element instances and consistency relations (written in OCL). In general, it is impossible to ensure that an OCL expression matches a specification -- undecidability: OCL can be transformed to first-order logic~\cite{beckert2002translating}.
    %\item Limitation also concerns relations that are recurring in practice, e.g. reasoning about strings, such as OCL operations \texttt{toUpper} and \texttt{toLower}, which cannot be easily transferred to state-of-the-art SMT solvers like Z3.
    %\item SMT solvers use heuristics, so it is not possible to clearly state which kinds of relations can be analyzed and which may not.
    %\item Problematic are, among others, operations on collections (transferred to quantifiers which are hard to analyze).
    %\item Number of variables is relevant, because they are existentially quantified, so more 
%\end{itemize}

%%
%% Recall results
%%
Applying the prototypical implementation of our approach to the scenarios introduced in \autoref{sec:evaluation:methodology} led to the result that twelve of the 15 compatible transformation networks were correctly classified as compatible, whereas three were not.
This leads to a recall value of 80\%.
\begin{align*}
    &
    Recall = \frac{\mathtext{true positives}}{\mathtext{true positives + false negatives}} = \frac{12}{12+3} = 0.8
\end{align*}
%
% Individual scenario discussion
%
% False negatives: Scenarios 8/18/19
% \begin{itemize}
%     \item All scenarios were not identified compatible although they are
%     \item In all cases, the SMT solver returned \emph{unknown} although he should have returned \emph{unsat}, thus an actually redundant consistency relation was not removed and the consistency relation set was not considered compatible by mistake
%     \item In all cases, set operations were involved
%     \item More precisely, in scenario 8 a precondition checks that an element is included in the intersection of two set literals, which the solver was not able to check properly
%     \item In Scenario 18, transitive inclusion of sets was defined, which the solver was not able to check properly
%     \item Scenario 19 is comparable to scenario 18 but considers role names of classes with equivalent identifiers, which the solver was also not able to check properly.
% \end{itemize}
The three scenarios that were not classified, although they are actually compatible, are Scenarios 8, 18 and 19 from \autoref{tab:scenarios}.
In all cases, the SMT solver returned \emph{unknown}, although it should have returned \emph{unsatisfiable}.
In consequence, in each case an actually consistent consistency relation was not removed, thus the set of relations was not considered compatible although it is.
More precisely, in scenario 8, a precondition ensures that an element is included in the intersection of two set literals, which the solver was not able to check properly.
Scenarios 18 and 19 were problematic due to comparable reasons.
While in Scenario 18 the transitive inclusion of sets was defined, Scenario 19 considers role names of classes with equivalent identifiers, which both the solver was not able to check properly.
All observed false negatives were due to reasons of undecidability of the translation of OCL constructs to first-order logic.

%%
%% Summary: Limitations arise from SMT solver
%%
To summarize, we found that basic operations on primitive data types, even with non-trivial constraints involving integer equations and string operations, were treated correctly.
More complex operations and structures requiring many quantifiers led to unprovability by the SMT solver, especially concerning collection operations and role names.
%In consequence, the reasons for conservative behavior in all cases were limitations of the chosen SMT solver approach, rather than the concept of either the formalized or the operationalized approach.
Thus, the approach is especially applicable for consistency relations concerning attributes and primitive types.
However, this limitation does only concern the chosen SMT solver approach, but neither the concept of operationalization nor of the formal framework behind it.
We did especially not find a scenario, in which our definition of left-equal redundancy was too strict for proving compatibility.


\subsection{Discussion}

%%
%% Benefits of the approach
%%
The presented approach aims to support developers of transformation networks to independently develop individual transformations, i.e., parts of the network, without aligning the consistency relations on which the transformations are based a priori, but allows them to check their compatibility during or after development.
For that reasons, it provides a benefit by automating a process that currently requires manual effort by either aligning consistency relations with each other or by defining test cases, which are able to validate but not to verify compatibility, i.e., which cannot make any all-quantified statements about compatibility.
Even if the approach had a high degree of conservativeness, the approach would be beneficial for the combination of independently developed transformation.
First, there is still a chance that the approach is able to prove compatibility for a given set of relations.
Second, if the approach is not able to prove compatibility, it may still find some redundant relations and thus reduces the effort for the user to investigate the remaining relations for contradictions.
It would even be possible to define an interactive approach, combining the removal of redundant relations by proof and by user decision, as we will propose in \autoref{sec:futurework:compatibilityprocess}.
%
In the following, we discuss threats to the validity of our evaluation and discuss limitations of both the approach and our evaluation.

%\subsubsection{Algorithmic Degrees of Freedom}
%Discuss different possibility for decomposition and problem regarding "false" decompositions.


\subsubsection{Threats to Validity}

Although we designed our evaluation in a way such that it gives us appropriate insights on correctness and applicability of the approach, there may be limitations regarding its internal and external validity. %, especially arising from the case studies.

%\paragraph{Internal Validity}
\paragraph{Scenario Selection}
We developed the scenarios specifically for the evaluation of our approach.
Due to that reason, they may not be sufficiently representative for actual transformation networks. 
However, the scenarios were specifically designed to test different aspects of the approach.
They represent an extensive set of consistency relations and especially different types of relations, also considering edge cases that may be rare in practical scenarios.
That even provides a benefit regarding practical consistency relation specifications.

\paragraph{Scenario Complexity}
The scenarios only comprise OCL constructs that are currently supported by our approach.
This may be a bias, because unsupported constructs are not covered by the evaluation.
However, the algorithm would not deliver any results in such scenarios anyway, thus applying it to them would not give further insights.
Additionally, the unsupported constructs are only a limitation of the current implementation and not a conceptual limitation of the approach.
Finally, the limitation in complexity of the relations may also lead to the fact that we do not cover cases that actually occur in practice but for which our definition for redundancy is too strong to prove compatibility.
This is an actual limitation that has to be considered in further evaluations.

\paragraph{Scenario Size}
The considered scenarios are rather small, as they only consider up to four metamodels and only few consistency relations.
Actual consistency relations will involve larger metamodels and consistency relations.
However, the inductive characteristics of our approach makes it independent from the number of metamodels and relations to consider.
One property affected by the scenario size is the performance of the approach, which we discuss in \autoref{sec:evaluation:limitations}.

% In consequence, some of these scenarios may be rare in practice, such that 
% Scenario selection (artifical): Scenarios were specifically developed to test different aspects of the operationalized approach. Thus, they may not be representative for actual specifications used in transformation networks.
% But: Scenarios provide an extensive set of relations, concerning different types of relations and as different scenarios as possible. Some of the scenarios may be rare in practice, even giving a benefit explicitly specifying them.
% Discuss: Especially affects appropriateness of redundancy definition (relations not complex enough)

% Scenario size: Scenarios are rather small (few metamodels and relations). Actual scenarios might involve more metamodels and/or relations
% But: Inductive character of the approach makes it independent from the number of metamodels and relations.
% Size may only affect performance.

% Scenarios limited to supported constructs: May be a bias that unsupported QVT-R and OCL operations are not covered by the scenarios.
% But: Would not make sense, since the algorithm does not deliver any results for such relations.
% But: Is not a conceptual limitation but only a limitation of the current implementation.

\paragraph{Conclusion}
In consequence, our evaluation only gives an initial indicator for the applicability of our approach due to the limited set and complexity of scenarios.
To improve evidence in external validity, applying the approach to further, more practical transformation networks would be beneficial.
However, acquiring such a networks is difficult.
At least existing networks contain transformations that are aligned with each other and thus do not allow to validate cases in which consistency relations are actually incompatible.
It may be possible to reduce that problem by taking existing sets of consistency relations and manually extending them with either redundant or incompatible consistency relations, checking whether the approach is able to correctly remove the added redundant relations or detect incompatibility.

%\paragraph{External Validity}

% Only initial results regarding correctness and applicability due to limited set of scenarios.
% But: Conservativeness is proven.
% But: Hard to acquire further transformation networks, especially those in which the transformations were not developed together and are thus aligned. Problem may be reduced by simulated redundancy, i.e., taking a set of compatible consistency relations and adding redundant one, validating whether our approach is able to remove them again and deliver a compatible network.



\subsubsection{Limitations}
\label{sec:evaluation:limitations}

Current limitations of our approach especially arise from its operationalization and the limitations of SMT solvers.
Additionally, we are currently only able to argue for the benefits and performance of our approach, but further evaluation would be necessary to validate these arguments.

\paragraph{Operationalized Approach}
The operationalized approach has both fundamental and technical limitations.
First, SMT solvers are limited in a way that they are not able to analyze all types of expressions regarding satisfiability.
In consequence, even if all kinds of \qvtr respectively OCL constructs can be transformed into appropriate logic formulae, it may not be able to check them for satisfiability, as we have seen in the applicability evaluation.
Second, we do not yet provide a translation for all kinds of OCL constructs to logic formulae, such that not all \qvtr relations are supported.
However, this is a technical limitations that can be solved by implementing these translations.

\paragraph{Benefits Evaluation}
We did not provide an evaluation for the claimed benefits of our approach.
This is due to two reasons.
First, we already argued why the approach provides a benefit anyway due to being fully automated and not requiring further input.
Second, to the best of our knowledge, there are no competitive approaches to compare our approach with.
In consequence, only an empirical study in which the approach is practically applied would give further insights to its benefits for a user, apart from the obvious benefits given by the automation of a currently manual process.
%No benefits analyzed: To the best of our knowledge no other approaches to compare with. Claim that automating a process that would have to be performed manually always provides a benefit.

\paragraph{Performance Evaluation}
We did neither formally evaluate nor measure performance of our approach.
If the approach required to much time to be executed on a set of transformations, its applicability would be reduced.
SMT solvers, such as the used Z3 solver, depend on heuristics, which makes their performance unpredictable.
Thus, it would be important to evaluate performance of the approach in a case study.
In our case study, %introduced in \autoref{sec:evaluation:methodology}, 
we did observe any time-consuming scenarios.
However, transformation networks with more and larger transformations and especially many cycles need to be investigated to make generalizable statements on the performance.


\iffalse
Functional correctness:
    - Theoretical evaluation based on concepts and definitions of this paper
    -> Functioning of the procedure, algorithms
    
Applicability:
    - Empirical evaluation based on an implementation of the decomposition procedure
    - Interpretation of test results + metric achieved results against expected results
    -> Example scenarios, execution results
\fi

\end{copiedFrom} % SoSym MPM4CPS




\begin{copiedFrom}{ICMT}

    \todo{Unterschied Klassifizierung/Kategorisierung: Klassifizierung benötigt Klassifizierungsdimension, Kategorisierung nicht}

\begin{copiedFrom}{ICMT}
%FORMERLY: \section{Issues in Networks of Bidirectional Transformations}
%\label{sec:classification}

In this section, we %first identify and 
categorize potential \emph{failures} that can occur when executing \acp{BX} in a network to preserve consistency.
We then consider \emph{mistakes} that a developer can make and that lead to \emph{faults} in the specifications of consistency and its preservation.
\todo{Heißt der folgende Satz nicht, dass auf jeder Ebene genau ein Typ von Fehler auftreten kann?}
We derive them from the specification levels introduced in \autoref{chap:properties:levels}, as each kind of mistake is specific for one of those levels.
We finally relate the mistakes to the failures that can occur while executing the operationalization of a faulty consistency specification.
That categorization forms our contribution \ref{contrib:issues}.
In the following, we only discuss failures and their causing mistakes, but no strategies to solve or avoid them.
Such strategies are discussed in \autoref{chap:prevention}.
%The identification and categorization in this section is based on argumentation. To show the correctness of identified mistakes, failures and their dependencies, we provide an appropriate evaluation in \autoref{sec:evaluation}.

%\todoHeiko{Introduce mistake, fault, failure} 

% \begin{itemize}
%     \item Define three essential abstraction levels in the development process
%     \item Levels depend on each other, so \emph{fulfillment} on one level is mandatory to investigate the next level
%     \item Mistakes on all levels may introduce failures in the execution of the operationalization of consistency constraint preservation
%     \item We summarize potential failures, identify their causes (mistakes and faults) and then categorize and relate them
% \end{itemize}

\section{Potential Failures}
\label{chap:errors:failures}

Mistakes in the specification of consistency, no matter on which of the specification levels, % (\autoref{sec:process:levels}),
can lead to failures when executing the preservation of consistency according to that specification. % on an actual system. 
Before identifying the causal mistakes, we first categorize the types of potential failures into three categories. We depict them in \autoref{fig:correctness:categorization}.

\begin{figure}
    \centering
    \input{figures/correctness/categorization.tex}
    \caption{Categorization and Dependencies of Mistakes, Faults and Failures}
    \label{fig:correctness:categorization}
\end{figure}

First, consistency preservation can fail by \textbf{resulting in an inconsistent state}. This can either occur \emph{deterministically} or \emph{non-deterministically}, if the result depends on the execution order of the consistency preservation specifications.

Second, consistency preservation can fail by \textbf{not terminating}. This can either manifest in an \emph{alternating loop}, when a feature, e.g., an attribute, alternates between two or more values, or in a \emph{diverging loop}, when at least one feature value diverges, e.g., a number counting up or a string being repeatedly appended.

Third, consistency preservation can result in \textbf{duplications}. \emph{Multiple instantiation} can occur because different consistency preservation specifications instantiate an element multiple times, although all of them represent the same element. % and thus should be the same. 
For example, an element is created by transformations $\mathcal{M}_1 \rightarrow \mathcal{M}_2 \rightarrow \mathcal{M}_3$ and another is created by transformation $\mathcal{M}_1 \rightarrow \mathcal{M}_3$, although there should be only one element.
\emph{Multiple referencing} can occur due to the same reason because an element is inserted into a reference or attribute list several times, although it should be inserted only once. 
%Such duplications are a special kind of termination in inconsistent states.


% If mistakes are made during the specification of consistency, no matter on which of the levels introduced in \autoref{sec:process:levels}, this can finally lead to failures in the consistency preservation executed on an actual system. Before identifying the causing mistakes, we first give an overview on the types of failures that may occur and separate them into three categories.\\[-0.7em]

% \compactsubsection{Termination in inconsistent states}
% \begin{enumerate}[topsep=4pt]
%     \item \emph{Deterministic:} The consistency preservation process can deterministically terminate in a state that is not consistent. % wrt. the defined consistency specification.
%     \item \emph{Non-deterministic:} Consistency preservation can non-deterministically terminate in an inconsistent state, depending on the execution order of the binary consistency preservation specifications. %in which the partial consistency preservation rules are executed.
% \end{enumerate}

% \compactsubsection{Non-termination}
% \begin{enumerate}[resume, topsep=4pt]
%     \item \emph{Alternating loops:} Consistency preservation can be non-terminating, alternating between two or more values in at least one feature (e.g. a number or a String alternating between two values).
%     \item \emph{Diverging loops:} Consistency preservation can be non-terminating, having at least one feature with a diverging value (e.g. a number counting up or down, a String being always appended).
% \end{enumerate}

% \compactsubsection{Duplications}
% \begin{enumerate}[resume, topsep=4pt]
%     \item \emph{Multiple instantiation:} An element can be instantiated multiple times by different consistency preservation specifications, although all of them represent the same element and thus should be the same. E.g. an element is created by transforamtions $\mathcal{M}_1 \rightarrow \mathcal{M}_2 \rightarrow \mathcal{M}_3$ and another is created by transformations $\mathcal{M}_1 \rightarrow \mathcal{M}_3$, although the same element is meant.
%     \item \emph{Multiple referencing:} An element may also be inserted into a non-containment reference or an attribute list several times, although the same element is meant, within the same situations as multiple instantiation can occur.
% \end{enumerate}


\section{Mistakes and Faults}
\label{chap:errors:mistakes}

%\todoErik{Ich dachte immer, \enquote{Mistakes} machen nur Menschen}
Developers or the transformation engine can make different kinds of mistakes on each of the specification levels, which lead to faults in the specification and finally to different kinds of failures during consistency preservation.
In the following, we derive mistakes and faults from the specification levels, depicted in \autoref{fig:correctness:categorization}.

\subsection{Global Level}
Regarding global consistency specifications for a set of model types, two basic mistakes can be made. 
These mistakes concern compliance of the defined consistency specification with the actual notion of consistency between the involved model types.
First, a specification can be incomplete (\emph{underspecified}), which means that some consistency constraints are missed. 
As a result, the consistency specification according to \autoref{def:consistency_specification} would contain more tuples of models than are actually consistent to each other. 
%%As a result, if one would define the consistency specification according to \autoref{def:consistency_specification}, more tuples of models would be in the relation than are actually consistent to each other. 
%Incomplete consistency specifications can lead to \emph{false positives}, when investigating whether a given tuple of models is consistent or not.
Another potential mistake are too restricted (\emph{overspecified}) consistency specifications, which means that additional, faulty consistency constraints are considered. 
As a result, actually consistent tuples of models would be missing in the consistency specification according to \autoref{def:consistency_specification}. 
%As a result, if one would define the consistency specification according to \autoref{def:consistency_specification}, actually consistent tuples of models would not be in the relation. 
%This %, in contrast, 
%can lead to \emph{false negatives}, because actually consistent models are identified as inconsistent.

\begin{figure}[bt]
    \centering
%    \includegraphics[angle=-90, width=\textwidth]{figures/levels_overview.pdf}
    \input{figures/correctness/mistakes_specification_levels.tex}
    \caption{Examples for Mistakes on Different Specification Levels}
    \label{fig:correctness:mistakes_specification_levels}
\end{figure}

\subsection{Modularization Level}
When developers modularize the global consistency specification by defining binary consistency specifications, these modular specifications can be non-compliant with the global one. 
Two kinds of mistakes, similar to those at the global level, can be distinguished, regarding compliance of modular and global specifications. %, but regarding compliance of modular and global specifications rather than between the global specification and the actual notion of consistency.
First, modular consistency specifications can be incomplete (\emph{underspecified}), so that there are global constraints which are not covered by them. 
The modular consistency specifications $\mathit{CS}_{1,2}$, $\mathit{CS}_{2,3}$ and $\mathit{CS}_{1,3}$ in \autoref{fig:correctness:mistakes_specification_levels} are incomplete iff
%For three model types $\mathcal{M}_1, \mathcal{M}_2$ and $\mathcal{M}_3$ with a global consistency specification $\mathit{CS}$, the binary specifications $\mathit{CS}_{1,2}$, $\mathit{CS}_{2,3}$ and $\mathit{CS}_{1,3}$, as depicted in \autoref{fig:levels_overview} are underspecified iff 
%\todoHeiko{Hier die Grafik aus dem Level-Kapitel übernehmen}
\begin{align*}
    & \exists M_1, M_2, M_3 : \\
    & \hspace{1em} (M_1, M_2) \in \mathit{CS}_{1,2} \land (M_2, M_3) \in \mathit{CS}_{2,3} \land (M_1, M_3) \in \mathit{CS}_{1,3} \land (M_1, M_2, M_3) \not\in \mathit{CS}
\end{align*}
This finally leads to \emph{false positives} when investigating whether a given tuple of models is consistent regarding the global specification. %as actually inconsistent models (regarding the global specification) are identified as consistent. %investigating whether a given set of models is consistent regarding the global specification or not, because actually inconsistent models regarding $\mathit{CS}$ are consistent according to all modular relations $\mathit{CS}_{1,2}$, $\mathit{CS}_{2,3}$ and $\mathit{CS}_{1,3}$.
%This finally leads to \emph{false positives} when investigating whether a given set of models is consistent regarding the global specification or not, because actually inconsistent models regarding $CS$ are consistent according to all modular relations $\mathit{CS}_{1,2}$, $\mathit{CS}_{2,3}$ and $\mathit{CS}_{1,3}$.
%\todoHeiko{Das folgende eher zu Avoidance Strategy? Überlapp vorhanden?}
%Such incomplete specifications can especially occur if constraints between two types of models are not expressed at all (so the consistency specification covers all model pairs), but are only transitively defined over two or more other relations. 
%For example, if $\mathit{CS}_{1,3}$ shall be omitted and transitively expressed across $\mathit{CS}_{1,2}$ and $\mathit{CS}_{2_3}$, the following must hold:
% \begin{align*}
%     & \forall M_1 \in \mathcal{M}_1 : \forall M_2 \in \mathcal{M}_2 : \forall M_3 \in \mathcal{M}_3 : \\
%     & \hspace{1em} \mathit{CS}(M_1, M_2, M_3) \iff \mathit{CS}_{1,2}(M_1, M_2) \land \mathit{CS}_{2,3}(M_2, M_3)
% \end{align*}
%\begin{align*}
    %& \forall M_1, M_2, M_3 : (M_1, M_2, M_3) \in \mathit{CS} \Leftrightarrow (M_1, M_2) \in \mathit{CS}_{1,2} \land (M_2, M_3) \in \mathit{CS}_{2,3}
%\end{align*}
%If this transitive relation misses or is even unable to express certain direct constraints, inconsistent models would be idenitified as consistent. %\todoHeiko{Das transitive muss man wohl an einem Beispiel erklären, am besten Ref. zu Intro}
Modular consistency specifications cannot only be incomplete because of an actual specification mistake, but also because of $n$-ary relations on the global level that cannot be expressed by a set of binary relations.
We excluded that case by our assumption made in \autoref{chap:properties:levels}, as otherwise a modularization into binary relations would not be possible at all.
If such cases have to be supported, the modularization would have to be extended to also consider $n$-ary relations.

Second, a modular specification can be too restricted (\emph{overspecified}) regarding the global consistency specification if additional constraints are added. 
The modular consistency specifications in \autoref{fig:correctness:mistakes_specification_levels} are overspecified iff
\begin{align*}
    & \exists M_1, M_2, M_3 : \\
    & \hspace{1em} (M_1, M_2, M_3) \in \mathit{CS} \land \big[ (M_1, M_2) \not\in \mathit{CS}_{1,2} \lor (M_2, M_3) \not\in \mathit{CS}_{2,3} \lor (M_1, M_3) \not\in \mathit{CS}_{1,3} \big]
\end{align*}
In \autoref{fig:correctness:mistakes_specification_levels}, omitting the dashed relation in $\mathit{CS}_{2,3}$ would lead to such an overspecifiation.
Overspecifications lead to additional constraints regarding the global specification, but also, and more severe, to contradicting constraints regarding other modular specifications.
In case of contradictions, the modular consistency specifications cannot be fulfilled at the same time.
In such a case, the graph of consistency relations %, as shown in \autoref{fig:mistakes_specification_levels}, 
would contain no cylces, i.e. sets of models that are consistent to each other.
We have discussed an example for such contradicting specifications %in the motivating example 
in \autoref{chap:properties:levels}, where constraints for transferring an employee name contradicted. % contains contradicting constraints for transferring the name. % to other types of models.
%In this case, when several binary specifications are combined to keep multiple models consistency, the resulting fault are incompatible binary specifications in the sense that different relations cannot hold at the same time because they are based on different global consistency specifications.
Such mistakes lead to \emph{false negatives} as actually consistent models (regarding the global specification) are identified as inconsistent. %, when investigating whether a given set of models is consistent regarding the global specification or not, because actually consistent models regarding $\mathit{CS}$ are not consistent according to the modular relations $\mathit{CS}_{1,2}$, $\mathit{CS}_{2,3}$ and $\mathit{CS}_{1,3}$.

%\begin{itemize}
    %\item inadequate structure
    %\item missing knowledge about other modular relation
%\end{itemize}

\subsection{Operationalization Level}
The types of mistakes that can be made at the operationalization level are different from those at the other levels, because this level does not concern the definition of consistency specifications (\autoref{def:consistency_specification}), but of consistency \emph{preservation} specifications (\autoref{def:consistency_preservation_specification}).
Such specifications are faulty if no composition of them exists that returns a consistent tuple of models for each possible change. % it does not lead to a consistent state after making modifications to a consistent tuple of models.
In \autoref{fig:correctness:mistakes_specification_levels}, an exemplary application of a single consistency preservation specification is depicted that leads to models that are not consistent according to the (global and modular) consistency specifications.
%If no concatenation of CPSs exists that finally returns a consistent set of models for each possible change, the specifications are faulty.
Let $\mathcal{CPS}$ be a set of consistency preservation specifications  %, e.g. $\mathcal{CPS} := \{\mathit{CPS}_{1}, \ldots, \mathit{CPS}_{m}\}$
for the binary consistency specifications $\mathcal{CS}$ % := \{\mathit{CS}_{1,2}, \mathit{CS}_{2,3}, \mathit{CS}_{1,3}\}$ %(where there can be more than one consistency preservation specifications for each consistency specification) 
%in \autoref{fig:mistakes_specification_levels}. %, metamodels $\mathcal{M}_0, \ldots, \mathcal{M}_n$, 
and
let $\mathfrak{M}_{\mathcal{CS}}$ be the set of model tuples that are consistent regarding $\mathcal{CS}$ (cf. \autoref{chap:properties:terminology}). 
The consistency preservation specifications are faulty iff
% \begin{align*}
%     & \exists M_0, M'_0 \in \mathcal{M}_0, M_1, M'_1 \in \mathcal{M}_1, M_2, M'_2 \in \mathcal{M}_2 : \forall \mathit{CPS}_0, \ldots, \mathit{CPS}_k \in \mathcal{CPS} : \\
%     & \hspace{1em} ((M_0, M''_0), (M_1, M''_1), (M_2, M''_2)) = \mathit{CPS}_0 \circ \dots \circ \mathit{CPS}_k((M_0, M'_0), (M_1, M'_1), (M_2, M'_2)) \\
%     & \hspace{1em} \Rightarrow \exists \mathit{CS}_{i,j} \in \mathcal{CS} : \neg \mathit{CS}_{i,j}(M''_i, M''_j)
% %    & \exists M_0, M'_0 \in \mathcal{M}_0, \ldots M_n, M'_n \in \mathcal{M}_n : \nexists \mathit{CPS}_0, \ldots, \mathit{CPS}_k \in \mathcal{CPS} : \\
% %    & \hspace{1em} ((M_0, M''_0), \dots, (M_n, M''n)) := \mathit{CPS}_0 \circ \dots \circ \mathit{CPS}_k((M_0, M'_0), \dots, (M_n, M'_n)) \\
% %    & \hspace{1em} \land \exists \mathit{CS}_{i,j} \in \mathcal{CS} : \neg \mathit{CS}_{i,j}(M''_i, M''_j)
% \end{align*}
\begin{align*}
    & \exists (M_1, \dots, M_n) \in \mathfrak{M}_{\mathcal{CS}}, (M'_1, \dots, M'_n) \in \mathcal{M}_1 \times \dots \times \mathcal{M}_n: \forall \mathit{CPS}_1, \ldots, \mathit{CPS}_k \in \mathcal{CPS} : \\
    %& \exists M_1, M'_1 \in \mathcal{M}_1, \dots, M_n, M'_n \in \mathcal{M}_n : \forall \mathit{CPS}_1, \ldots, \mathit{CPS}_k \in \mathcal{CPS} : \\
    & \hspace{1em} \mathit{CPS}_1 \circ \dots \circ \mathit{CPS}_k \big((M_1, M'_1), \dots, (M_n, M'_n) \big) = \big( (M_1, M''_1), \dots, (M_n, M''_n) \big)\\
    & \hspace{2em} \land \exists \mathit{CS}_{i,j} \in \mathcal{CS} : (M''_i, M''_j) \notin \mathit{CS}_{i,j}
%    & \exists M_0, M'_0 \in \mathcal{M}_0, \ldots M_n, M'_n \in \mathcal{M}_n : \nexists \mathit{CPS}_0, \ldots, \mathit{CPS}_k \in \mathcal{CPS} : \\
%    & \hspace{1em} ((M_0, M''_0), \dots, (M_n, M''n)) := \mathit{CPS}_0 \circ \dots \circ \mathit{CPS}_k((M_0, M'_0), \dots, (M_n, M'_n)) \\
%    & \hspace{1em} \land \exists \mathit{CS}_{i,j} \in \mathcal{CS} : \neg \mathit{CS}_{i,j}(M''_i, M''_j)
\end{align*}
%\todoHeiko{Das ist nicht so schön, weil nicht klar ist, welche CS to welcher CPS gehört und so}
%This means that there can be changes for which no execution of consistency preservation specifications is able to properly restore consistency. 

In practice, mistakes at the operationalization level occur due to missing identification of equal elements in different consistency preservation specifications. 
In our motivational example (\autoref{fig:properties:motivational_example}), %consider that an employee is created in the scheduling system for an employee created in the task management system after introducing one in the personnel management system.
consider that an employee is created in the personnel management system, transformed to the task management system and from that to the scheduling system.
The additional direct specification between personnel management and scheduling system has to consider the already created employee rather than instantiating a new one.
%We consider %this case and 
%options to avoid such problems in \autoref{sec:avoiding:matching}.
%In our example, if a class is created in Java after creating a UML class for a ADL component through appropriate consistency preservation specifications and the consistency preservation specification between ADL and Java also defines the creation of a class in Java, it is necessary that the already existing class is considered rather than creating a new class. We will consider this case and options to avoid such problems in \autoref{sec:avoiding:matching}.

% \begin{itemize}
%     \item unknown connection of elements in consistency specifications
%     \item Really, really make an example here, to distinguish from modularization level!!
% \end{itemize}

%In the following, we call all mistakes on modularization and operationalization level \emph{interoperability issues}, as they are all concerned with modularized specifications that have to interoperate. 


\section{Categorization and Discussion}
\label{chap:errors:categorization}

\begin{figure}[tb]
    \centering
    \input{figures/correctness/failure_examples_employee.tex}
%    \includegraphics[angle=270, width=\textwidth]{figures/mistakes_examples_employee.pdf}
    \caption{Consistency Constraints on Metamodel Extract (top), Failure due to Mistake on Modularization Level (left), Failure due to Mistake on Operationalization Level (right)}
    \label{fig:correctness:mistake_effects_example}
\end{figure}

%We associated the mistakes presented in the previous section with the specification level they can occur on. Additionally, we summarized potential failures that can occur when executing final consistency preservation specification in the section before.
Although all failures occur during operationalization, the mistakes that lead to them can also be made at a higher specification level, such as the modularization or global level.
More importantly, each type of failure can be traced back to specific types of mistakes, or, vice versa, specific mistakes lead to specific kinds of failures.
\autoref{fig:correctness:mistake_effects_example} shows extracts of the three metamodels from our motivation, as well as consistency constraints between them.
There are two options for a constraint between personnel data and scheduling system.
The first option is contradictory to the one defined between personnel data and task management system, as already discussed in \autoref{chap:properties:levels}.
This demonstrates that contradictory constraints are a typical fault that can result from contradicting modular knowledge, when different persons define such constraints independently.
If, nevertheless, such a contradictory consistency specification is operationalized to a consistency preservation specification, the propagation of changes may never terminate.
This is shown in the left scenario in \autoref{fig:correctness:mistake_effects_example}, where
%Due to the contradicting constraints, 
the name is replaced repeatedly in an \emph{alternating loop} as indicated by the dashed arrows.

If no mistakes are made on the modularization level, so that no contradictions exist, %which especially means that the consistency specifications are free of contradictions, 
missing matching of equal elements in the consistency preservation specifications can still lead to duplicate element instantiations.
With the second option for the constraint in \autoref{fig:correctness:mistake_effects_example}, %no contradicting constraints and thus 
no mistakes on modularization level exist.
However, a missing matching of elements %in the consistency preservation specification 
can lead to the situation shown in the right scenario of \autoref{fig:correctness:mistake_effects_example}, in which two employees are instantiated across different transformation paths.
%We also demonstrated in the example that missing matching of equal elements in the consistency preservation specifications can lead to duplicate instantiations of elements.

These were two of several causal chains for mistakes and faults to resulting failures.
We give a full overview of those dependencies in \autoref{fig:correctness:categorization}.
Missing constraints lead to deterministic inconsistencies, because such inconsistencies are not modelled and thus resolved.
Additional consistency constraints do not lead to any actual failures, but reduce the set of consistent models. 
The only consequence is that consistency preservation does not consider models that would actually be consistent.
Contradicting constraints, which can arise from a faulty modularization, are more severe, as we have seen in the example:
They can either lead to non-deterministic inconsistencies, e.g., depending on the execution order of consistency preservation specifications, or to loops that alternate or diverge values.
Finally, the missing element matching at the operationalization level can lead to multiple instantiations, as we have seen in the example, or multiple insertions. %, if elements are added to a multi-valued reference multiple times.

%\todoHeiko{Tun wir das wirklich? Oder nur ein Level?}
%In the following, we discuss strategies to avoid mistakes at the different levels.
%Afterwards, we evaluate whether our identified categorizes of mistakes, faults and failures and their dependencies are actually correct.

%Categorize the detected Causes/Mistakes into three categories, which map to the steps identified in the first subsection. Two categories have to be resolved by user and, especially, are only resolvable in the moment when concrete transformations are combined (explain why!). One category can be solved by applying appropriate pattern, explained in the next section. Each category resolution is an assumption of the next (e.g. pattern matching does not make any sense when transformations are incompatible or at least the failures than can occur may differ).

\end{copiedFrom} % ICMT

\section{Prevention Strategies}

We have systematically constructed the categorization in \autoref{chap:errors} from the potentials for mistakes that are induced by the different specification levels. % we identified (\autoref{sec:process:levels}).
To further improve evidence regarding completeness and correctness of %the identified mistakes, resulting failures and their dependencies, we validate that 
our categorization, we validate it in a case study as our contribution~\ref{contrib:evaluation}.
The goal is 
 to show completeness of the identified mistakes and failures, and
 to investigate correctness of the dependencies between them. % mistakes and resulting failures. %,
 %and to show appropriateness of the strategies for avoiding mistakes presented in \autoref{sec:avoiding}.

% Evaluation goals:
% \begin{itemize}
%     \item Completeness of identified mistakes/failures
%     \item Correctness of dependencies between failure types and mistakes
%     \item Appropriateness of matching strategy for avoiding operationalization issues
% \end{itemize}

\subsection{Case Study}
The evaluation is based on a case study developed for the Ecore-based \textsc{Vitruvius} framework~\cite{kramer2013b} for consistent system development, which is available on GitHub~\cite{vitruvFrameworkGithub}.
\textsc{Vitruvius} uses incremental, delta-based consistency preservation. 
It records atomic changes in models and executes consistency preservation specifications, according to \autoref{def:consistency_preservation_specification}, to inductively preserve consistency.
Those specifications are written in the Reactions language~\cite{klare2016b}, which is a language for unidirectional transformations at the operationalization level.

The case study is based on consistency between UML class models, instances of the \ac{PCM}, which is an architecture description language for performance prediction~\cite{reussner2016b}, and Java code.
For these metamodels, different persons have independently developed transformations~\cite{kramer2017a}, especially without knowing about the other transformations with which they shall be combined.
This made the specifications prone to mistakes at the modularization and operationalization level.
The specifications are available on GitHub~\cite{vitruvCBSEGithub}.
For the evaluation, we employ the pairs of unidirectional specifications between \ac{PCM} and UML, as well as between UML and Java.
Although this induces only two bidirectional specifications, we have four transformations since both directions of the transformations have been specified independently.
They have to interoperate correctly, may also contradict, and need to perform element matching.
% In consequence, this is not a drawback or threat in contrast to a case study of at least three independently developed \acp{BX} that are prone to interoperability issues (apart from the fact that it is hard to find such a case study).
Thus, our scenario is prone to the same mistakes as a scenario with three or more \acp{BX}.

The transformations realize rather trivial constraints between UML and Java.
Most elements are mapped one-to-one, whereas multi-valued parameters and associations are mapped to collection types in Java.
The relations between \ac{PCM} and UML were proposed by \textcite{langhammer2015a}.
Interfaces are equally represented, \ac{PCM} components and data types are mapped to classes in UML.
%Interfaces and data types are mapped to their counterparts in the different metamodels.
\ac{PCM} components contain \acp{SEFF}, which are an abstraction of their behavior specification used for performance prediction. 
Those \acp{SEFF} are mapped to methods in UML and Java.
In total, the transformations between \ac{PCM} and UML react to 57 change types in \ac{PCM} and 65 change types in UML, and the transformations between UML and Java react to 66 change types in UML and to 48 change types in Java to restore consistency in the other model.
%consist of reactions to 57 change events in \ac{PCM} that restore consistency to UML, reactions to 67 change types in UML that restore consistency in \ac{PCM}, reactions to 55 change types in UML that restore consistency in Java, and reactions to 48 change types in Java that restore consistency in UML.

In total, we have used 187 test cases that perform different kinds of relevant fine-grained changes in instances of all metamodels, such as insertions, modifications and deletions of all types of elements that have to be kept consistent.
Additionally, we have simulated the construction of the Media Store system~\cite{strittmatter2016a}, which is a sophisticated case study system for the \ac{PCM}.
This system is available as a \ac{PCM} model as well as Java code.

% \begin{itemize}
%     \item Used environment: Implementation in the Ecore-based Vitruvius framework \cite{kramer2013b}~\footnote{\url{http://vitruv.tools}} for consistent system development
%     \begin{itemize}
%         \item incremental, inductive, delta-based consistency preservation: recording atomic changes, executing consistency preservation specifications for them, as defined in \autoref{def:consistency_preservation_specification}
%         \item specifications in the Reactions language (definition on operationalization level)
%     \end{itemize}
%     %\item Application strategy: batch, Depth-first, finally irrelevant, as same mistakes can occur with any strategy
%     \item Metamodels and consistency preservation specifications:
%     \begin{itemize}
%         \item UML, PCM and Java
%         \item Existing specifications: UML $\leftrightarrow$ Java, PCM $\leftrightarrow$ UML, PCM $\leftrightarrow$ Java
%         \item Independently developed, without knowledge about combined execution $\rightarrow$ black-box development
%         \item So during combined execution mistakes on modularization and operationalization level possible
%     \end{itemize}
%     \item Evolution scenarios:
%     \begin{itemize}
%         \item Test suite of \FIXME tests each making different kinds of small changes
%         \item Simulation of construction of example system: Media Store \cite{strittmatter2016a, reussner2016b}, a case study for PCM with existing specifications in PCM and Java
%     \end{itemize}
% \end{itemize}


\subsection{Methodology}

\subsubsection*{Process}
We executed the test cases on a transformation network, which we created as a combination of the existing transformations.
They were executed until no further changes occurred.
We then classified the occurring failures according to \autoref{chap:errors:failures}.
Based on our categorization in \autoref{chap:errors:categorization}, we traced back the failures to mistakes and fixed them according to the strategies discussed in \autoref{chap:prevention}.
Failures can be hidden by others: 
For example, an incompatible constraint may produce no failure because the scenarios fail earlier due to missing element matching or vice versa.
For this reason, we re-executed the process until no further failures occurred.
Finally, we applied the transformations to the more complex Media Store construction case to validate that all mistakes were fixed.

\subsubsection*{Measurements}
We measured the number of failures in each of the iterations.
We relate the number of failures that we were able to categorize to the total number of recognized failures ($\mathit{identifiedFailureRatio} = \frac{\mathit{\#\ of\ categorized\ failures}}{\mathit{\#\ of\ total\ failures}}$) to show completeness of the identified failure types.
This metric is rather weak, because it does not identify whether a failure is categorized correctly.
We therefore relate the total number of resolved failures, which are those that do not occur in the subsequent iteration anymore, to the number of detected failures ($\mathit{resolvedFailureRatio} = \frac{\mathit{\#\ of\ resolved\ failures}}{\mathit{\#\ of\ total\ failures}}$).
If a failure disappears after fixing the causing mistakes, the classification of the failure and also the relation to the causing mistake was correct. %, which is why this metric gives an indicator for the completeness of the identified failure types.
Therefore, this metric gives an indicator for both completeness of the identified failure types and the relation of mistakes to failures.

%To measure correctness of the dependencies between mistakes and failures, we relate the resolved mistakes to the detected failures ($\frac{\text{# of resolved mistakes}}{\text{# of detected failures}}$.
%If the number of resolved mistakes is lower than the number of failures, actually failures remain after fixing an mistake, which indicated that some relation between mistake and failure type was wrong.

%\todoHeiko{Wieder reinnehmen: Validierung der Strategien zum Element-Matching?}
%To validate the appropriateness of our strategy for avoiding mistakes due to missing element matching, we count the kinds of matching techniques according to \autoref{sec:avoiding:matching} that were used.


% This, on the one hand, 

% \begin{itemize}
%     \item Process
%     \begin{enumerate}
%         \item Combined execution of consistency preservation specifications on evolution scenarios
%         \item Identification of failures during execution / in resulting models
%         \item Fixing mistakes that lead to those failures (based on categorization in \autoref{sec:classification:categorization}
%         \item Re-execute scenario
%     \end{enumerate}
%     \item Important: the number of failures can increase or stagnate after an iteration, because although a failure disappears due to a fixed mistake, another failure can occur because of an mistake that was hidden by the previous one (esp. on different specification levels). E.g. specifications were incompatible ("Impl"-suffix), leading to a loop, so we fixed that and afterwards a multiple instantiation problems occurs, because now objects are mapped to the same object in another model, leading to a matching problem.
%     \item Interpretation
%     \begin{itemize}
%         \item What we measure is the number of failures regarding a specific type, so we relate all metrics to the number of identified failures
%         \item Completeness of the failure catalog: If failure cannot be categorized with our catalog in \autoref{sec:classification}, our identified failure types are incomplete: Metric: $\frac{\# of categorized failures}{\# of total failures}$
%         \item The previous metric is rather weak, because categorizing a failure does not mean that it is categorized correctly, so we try to fix the causing mistake and measure the success
%         \item If failure disappears after fixing mistake, the identified relation between mistake and failure is correct and the failure identification is correct; if failure does not disappear after fixing mistake, either the relation is only incorrect, the failure identification is incorrect or an actual mistake is missing: General failure-/mistake-independent Metric: $\frac{\# of resolved mistakes}{\# of detected failures}$
%         \item To see whether a certain relation between failure and mistake is wrong, for each failure/mistake: $\frac{\# of resolved mistakes of type}{\# of total failures of type}$; if this metrics is < 1, the failure is caused by a different than the identified mistake
%         \item Problem: several failures caused by one mistake
%     \end{itemize}
% \end{itemize}


\subsection{Results}

We had to perform two iterations of the previously described process.
In the first iteration, we faced failures due to mistakes at the operationalization level, whereas in the second iteration only failures due to remaining mistakes at the modularization level occurred.
We have tagged the states before and after the evaluation process in the GitHub repository~\cite{vitruvCBSEGithub}.

%\compactsubsection{Classification}

In the first iteration, all 187 tests failed.
The reason was that all transformations assumed that new elements are only created by the user or the transformation itself.
In consequence, we observed multiple instantiations and insertions in 187 cases, which we could trace back to 35 missing matchings of elements in the transformations.
After adding appropriate matchings, all these failures disappeared in a second iteration, so for the first iteration $\mathit{identifiedFailureRatio = resolvedFailureRatio = 1}$, since all detected failures were identified and resolved.

In the second iteration, 5 new failures occurred.
Three of them were diverging loops, which were caused by a namespace repeatedly prefixed to the name of classes, interfaces and enumerations in Java.
The causing mistakes were incompatible constraints: The Java model contains the fully qualified name of a class, whereas the UML model only contains the simple name, which was correctly propagated from UML to Java, but the namespace prefix was not removed in the opposite direction.
The two other failures were alternating loops, which were caused by alternations of element visibilities.
For methods and constructors, the visibilities were repeatedly changed due to an inconsistent mapping of visibilities from UML to Java and vice versa.
%After fixing the mistakes, two failures remained.
%Nevertheless, the reason for that were technical issues with the transformation engine due to its propagation of atomic changes.
%Since the original failures also disappeared in this case, we again have $identifiedFailureRatio = resolvedFailureRatio = 1$, since all detected failures were identified and resolved.
After fixing those mistakes, no failures remained.
So we again have $\mathit{identifiedFailureRatio = resolvedFailureRatio = 1}$, since all detected failures were identified and resolved. 

Summarizing, we were able to classify and resolve all failures in the case study and trace them back to mistakes with our classification in \autoref{chap:errors}.
This demonstrates the applicability of our categorization and is an indicator for the completeness and correctness of our catalog.
Most important, we did not find any failures that were caused by mistakes at a different specification level than we expected.
To further validate the catalog, we should apply it to further case studies.
It is however hard to find existing, independently developed transformations between at least three metamodels.
They would have to be developed in a schema similar to the one proposed by \textcite{kramer2016c}.
%\todoHeiko{Irgendwie noch sagen, dass wegen das wegen dem hohen Aufwand kein Threat to Validity ist? Oder kommt das nicht gut an?}
%\todoHeiko{Generell mehr Threats to Validity diskutieren?}


%\compactsubsection{Element Matching}

%In \autoref{sec:avoiding:matching}, we presented three levels of matching equal elements across different transformation paths.
%We used our case study to investigate, which of those levels are necessary in a practical scenario.

\end{copiedFrom} % ICMT


\section{Limitations}

\begin{itemize}
    \item No concurrent editing support. May partially be resolved by synchronizing transformations, however as we do not make assumptions to these current changes (rather than the assumptions we make to the transformations), the changes of two user can be conflicting. This has to be addressed -> future work
\end{itemize}



\section{Limitations}

Only binary relations/transformations rather than BX, but can be transferred

Only binary-definable relations (see Stevens): But, like above, insights can be transferred to networks of MX rather than BX

Considered only structural relations (see foundations): Need to investigate behavioral or extra-functional relations. Potentially those are not binary but multiary (see Dagstuhl), so extension to multiary necessary