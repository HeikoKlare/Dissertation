\section{Compatibility}
\label{chap:correctness_evaluation:compatibility}

\mnote{Evaluated properties}
In \autoref{chap:compatibility}, we have presented a formal notion of compatibility, a formal approach to prove it, and a practical realization of this approach for consistency relations defined in \gls{QVTR}.
The compatibility notion is well-defined, based on our formalization of transformation networks and a correctness notion for them.
The formal approach to validate compatibility of consistency relations of a transformation network is based on the insights that specific consistency relation trees are inherently compatible and that the addition and removal of consistency relations fulfilling a specific notion of redundancy preserve compatibility, thus removing redundant relations until a tree remains validates compatibility.
We have proven correctness of this formal approach with \autoref{theorem:redundancycompatibility}, \autoref{theorem:treecompatibility}, and \autoref{corollary:transitiveredundancycompatibility}, such that we do not need to evaluate it.
We thus focus on correctness of the practical realization of the approach as well as its applicability.
The presented evaluation is based on and in parts taken from the evaluation that we presented in previous work~\owncite{klare2020compatibility-report} and that was developed in the Master's thesis of \textowncite{pepin2019ma}.


\subsection{Goals and Methodology}

\mnote{Correctness and applicability}
A tool for proving compatibility could be easily integrated into the process of developing a transformation network in order to assist transformation developers, as it operates fully automated and thus introduces no further developer effort, and it improves the ability of the transformation network to find consistent models after changes.
Thus, the correctness and the applicability of the approach are of particular importance.

\begin{propertable}
    \rowcolors{1}{\secondlinecolor}{\firstlinecolor}
    \begin{tabular}{L{7.5em} L{\increasetoafour{23.5em}}}
        \toprule
        \rowcolor{\headinglinecolor}
        \goal{Compatibility} &
            Show that the analysis can be used by transformation developers to find incompatibilities in consistency relations of a transformation network. \\
        \question[eq:compatibility:correctness]{Correctness} & 
            \questiontext{Is compatibility always given if the analysis finds it?} \\
        \metric & 
            \metrictext{Precision: Ratio between true positives and the sum of true and false positives} \\
        \question[eq:compatibility:applicability]{Applicability} & 
            \questiontext{How often does the analysis not prove compatibility although it is given?} \\
        \metric & 
            \metrictext{Recall: Ratio between true positives and the sum of true positives and false negatives}\\
        \bottomrule
    \end{tabular}
    \caption[Goals, questions, metrics for compatibility]{Goals, questions, and metrics for compatibility evaluation.}
    \label{tab:correctness_evaluation:gqm_compatibility}
\end{propertable}

\mnote{Empirical evaluation in case study}
In the subsequently presented empirical evaluation in terms of a case study, we apply the practical realization of the approach to several sets of consistency relations, which are designed to be compatible or not according to \autoref{def:compatibility}.
We then apply the algorithm to prove compatibility to these consistency relation sets and analyze whether it properly identifies them to be compatible or not.
We denote the cases in which the algorithm proves compatibility as \emph{positives} and the ones in which it is not able to do so as \emph{negatives}.
Since the algorithm operates conservatively, a negative result does not mean that incompatibility is proven but only that compatibility could not be proven.
The goal of this evaluation, the answered questions, and the evaluated metrics are summarized in \autoref{tab:correctness_evaluation:gqm_compatibility}.

\mnote{Correctness evaluation}
First, the application of the algorithm to multiple scenarios allows us to validate correctness of the practical realization of the approach according to \autoref{eq:compatibility:correctness}.
Correctness of our approach means that it is able to classify a given set of consistency relations as compatible or otherwise does not reveal a result.
This especially means that it operates conservatively and does not classify a set of consistency relations as compatible although it is not.
The algorithm is thus not allowed to produce false positives, which is why we consider the \emph{precision} metric:
\begin{align*}
    &
    \mathvariable{precision} = \frac{\mathtextspacearound{true positives}}{\mathtextspacearound{true positives + false positives}}
\end{align*}
This metric needs to be $1$, as otherwise the algorithm produces false positives and would be incorrect per definition.
In consequence, correctness of the algorithm directly correlates with this metric.
Analyzing this metric serves as an indicator that the mapping of our formal approach and the underlying formalism to the practical approach realization and the used \gls{QVTR} language is correct, and especially that it operates conservatively.

\mnote{Applicability evaluation}
Second, the application of the algorithm to multiple scenarios allows us to validate its applicability according to \autoref{eq:compatibility:applicability}.
The approach uses a fully automated algorithm, thus it does not require any inputs apart from the \gls{QVTR} relations to check.
Applicability may thus be restricted if the algorithm operates too conservatively, i.e., if it produces false negatives too often.
In those cases, the algorithm operates actually correctly, but if it was not able to prove compatibility in most cases in which it is actually given, applicability would be reduced, as the usefulness of the results for a transformation developer is limited.
For that reason, we analyze the \emph{recall} metric:
\begin{align*}
    &
    \mathvariable{recall} = \frac{\mathtextspacearound{true positives}}{\mathtextspacearound{true positives + false negatives}}
\end{align*}
The higher the number of false positives, the more consistency relations could not be identified as compatible by the algorithm although they actually are, thus reducing the usefulness of the algorithm.
In consequence, applicability of the algorithm directly correlates with the recall metric.
For that reason, we analyze this metric and the reasons for the cases in which the algorithm was not able to 
prove compatibility, i.e., in which it produced false negatives.
In particular, it is relevant whether they are caused by conceptual issues of the formal approach, such as a too restricted notion of redundancy, or a limitation of the practical approach that may be fixed by a different implementation or a different realization approach.


\subsection{Prototypical Implementation}
\label{chap:correctness_evaluation:compatibility:implementation}

\mnote{Redundant relation detection}
The approach that we have presented in \autoref{chap:compatibility:practical_approach} resulted in the implementation of a prototype, which is available in a GitHub repository~\cite{decompositionGithub}.
The prototypical implementation is specific to \gls{QVTR} and \gls{OCL} expressions used in that language.
It expects a set of \gls{QVTR} transformations and returns a list of redundant \gls{QVTR} relations.
Thus, if removing the returned redundant relations from the initial set of transformations yields a set of transformations whose relations do not contain any cycles, i.e., if they form a consistency relation tree, compatibility is proven.
If cycles within the relations remain, compatibility could not be proven either because of an actual incompatibility or because of the algorithm not being able to find redundancies to prove compatibility.

\mnote{Input validation}
Additionally, the implementation validates the given inputs.
They may be invalid because of two reasons.
First, they can contain transformations that are not well-formed, i.e., they are syntactically incorrect. In that case, the transformation cannot be processed by the compatibility analysis algorithm at all.
Second, transformations can be well-formed but invalid, e.g., because two transformations have the same name or a \gls{QVTR} domain pattern uses a nonexistent class.
Although the algorithm can still be applied to such an input, it may not produce appropriate results, which is why such errors are displayed to the transformation developer when applying the algorithm in the parsing step.
Some errors, such as two transformations having the same name, could even be mitigated by automatically renaming them if such a clash occurs.
In the evaluation, we, however, only consider valid inputs anyway.
Finally, the implementation operates non-intrusively, thus not altering the transformations in any way.

\mnote{Modeling tool selection}
The selection of \gls{QVTR} for the practical realization and implementation of the approach was, on the one hand, driven by the recommendation of the \gls{MDA}~\cite{mda} to use \gls{QVTR} for defining transformations, and, on the other hand, by the fact that consistency relations are explicitly defined in \gls{QVTR}, especially in comparison to imperative languages.
We have based the implementation on the \gls{EMF} and its Ecore \metametamodel (see \autoref{chap:foundations:formalisms:ecore}) as one of the most common and technically mature modeling frameworks.
Within the \gls{EMF}, implementations of transformation languages are provided through the \emph{Eclipse MMT}~\cite{EclipseMMT} project.
In particular, the contained \gls{QVTd}~\cite{EclipseQVTd} language provides a parser for \gls{QVTR} transformations, which, in turn, uses \emph{Eclipse OCL}~\cite{EclipseOCL} as an implementation of \gls{OCL}.

\mnote{SMT tool selection}
For finding redundant relations, their \gls{OCL} constraints are transformed into logic formulae, whose satisfiability is then to be validated by an \gls{SMT} solver.
Many such solvers are based on SMT-LIB~\cite{smtlib2017}, which is an initiative that provides a common input and output language for \gls{SMT} solvers.
Our prototype uses the Z3 theorem prover~\cite{z32008}, which is an \gls{SMT} solver that can be used in Java code and supports a large number of theories.


\subsection{Case Study}
\label{chap:correctness_evaluation:compatibility:case_study}

\begin{propertable}
    \rowcolors{1}{\firstlinecolor}{\secondlinecolor}
    \renewcommand{\arraystretch}{1.2}%
    \begin{tabular}{L{1.4em} L{22.5em} C{5.7em}}
        \toprule
        \textbf{\#} & \textbf{Scenario Description} & \textbf{Compatible} \\
        \midrule
        1 & Three equal string attributes of three metamodels & yes\\
        2 & Six equal string attributes of three metamodels & yes\\
        3 & Concatenation of two string attributes & yes\\
        4 & Double concatenation of four string attributes & yes\\
        5 & Substring in a string attribute & yes\\
        6 & Substring in a string attribute with precondition & yes\\
        7 & Precondition with all primitive datatypes & yes\\
        8 & Absolute value of integer attribute with precondition & yes\\
        9 & Transitive equality for three integer attributes & yes\\
        10 & Inequalities for three integer attributes & yes\\
        11 & Contradictory equations for three integer attributes & no\\
        12 & Contradictory inequalities for three integer attributes & no\\
        13 & Constant property template items & yes\\
        14 & Linear equations with three integer attributes & yes\\
        15 & Contradictory linear equations for three int. attributes & no\\
        16 & Emptiness of various \gls{OCL} sequence and set literals & no\\
        17 & Equal string attributes for four metamodels & yes\\
        18 & Transitive inclusions in sequences & yes\\
        19 & Comparison of role names in three metamodels & yes\\
        \bottomrule
    \end{tabular}
    \caption[Example scenarios with compatibility classification]{Consistency relation scenarios and their compatibility. Taken from~\owncite[Tab.~3]{klare2020compatibility-report}.}
    \label{tab:correctness_evaluation:compatibility_scenarios}
\end{propertable}

\mnote{Scenarios of QVT-R transformations}
We have applied our prototypical implementation in a case study to $19$~scenarios, which are also available at GitHub~\cite{decompositionGithub}.
Each of these scenarios consists of three or four metamodels and comprises especially primitive data types and operations.
They contain pairwise transformations between the metamodels defined in \gls{QVTR}, more specifically its implementation \gls{QVTd}.

\mnote{Ground truth for scenarios}
The scenarios are listed in \autoref{tab:correctness_evaluation:compatibility_scenarios}.
It also depicts whether the relations of the transformations in these scenarios are compatible or not.
In total, $15$ of these scenarios contain compatible consistency relations according to \autoref{def:compatibility}, whereas the other four are incompatible.
Thus, we know for each of the scenarios by construction whether it is compatible or not, which constitutes the ground truth for our evaluations.
The application of the prototypical implementation to these scenarios yields the results \emph{positive} if it considers the relations compatible, or \emph{negative} if it was not able to prove compatibility.
Comparing these results with the ground truth in \autoref{tab:correctness_evaluation:compatibility_scenarios} allows us to identify them as true or false positives or negatives.

\mnote{Evaluation-specific scenarios}
The scenarios were specifically developed for the evaluation of the approach, thus reflecting as many kinds of relations as possible that can be expressed with \gls{QVTR} and also reflecting edge cases.
The implemented \gls{QVTR} relations used for the case study are also available in the GitHub repository containing the prototypical implementation~\cite{decompositionGithub}.


\subsection{Results and Interpretation}

\mnote{Classification of case study results}
We have applied the prototypical implementation of our practical approach introduced in \autoref{chap:correctness_evaluation:compatibility:implementation} to the case study explained in \autoref{chap:correctness_evaluation:compatibility:case_study}.
The results of the scenario classification as compatible or not by the implementation are summarized in \autoref{tab:correctness_evaluation:compatibility_results}.

\subsubsection{Correctness}

\mnote{Requirements for correctness by construction}
Correctness for the formal approach has been proven.
Since the practical approach is derived from this formal approach, correctness is also given by construction as long as the following requirements are fulfilled.
\begin{longenumerate}
    \item All relevant \gls{QVTR} relations are considered as consistency relations to be checked, i.e., all relations are represented in the property graph.
    \item All constructs referring to expressions in \gls{QVTR} relations have to be considered. \gls{QVTR} relations are defined using variables, so all constructs referring to these variables have to be considered. In particular, all template expressions need to be represented, namely property template items, preconditions, and invariants.
\end{longenumerate}
The construction of the approach presented in \autoref{chap:compatibility:practical_approach} ensures that these relevant elements are considered.
Additionally, the results of the case study further validate that we did not miss any relevant parts of \gls{QVTR} relations.

\begin{propertable}
    \renewcommand{\arraystretch}{1.2}%
    \begin{tabular}{L{7em} C{10em} C{6em}}
        \toprule
         & \textbf{Classified Compatible} & \textbf{Unclassified} \\
         \midrule
         \textbf{Compatible} & 12 & 3\\
         \textbf{Incompatible} & 0 & 4\\
         \bottomrule
    \end{tabular}
    \caption[Correctness of compatibility classification results]{Compatibility classification of scenarios depicted in \autoref{tab:correctness_evaluation:compatibility_scenarios} by our approach. Corrected from~\owncite[Tab.~4]{klare2020compatibility-report}.}
    \label{tab:correctness_evaluation:compatibility_results}
\end{propertable}

\mnote{Metric evaluation shows correctness}
The results depicted in \autoref{tab:correctness_evaluation:compatibility_results} show that the implementation did not yield any false positives.
Thus, the implementation operates conservatively as intended and does not identify consistency relations as compatible although they are not.
This results in a precision value of $1$:
\begin{align*}
    &
    \mathvariable{precision} = \frac{\mathtextspacearound{true positives}}{\mathtextspacearound{true positives + false positives}} = \frac{12}{12+0} = 1
\end{align*}

\mnote{Answer to evaluation question}
On the one hand, this indicates that the practical approach actually conforms to the formal approach, so that the correctness proof applies as well.
On the other hand, this indicates that the implementation is correct and does not miss any relevant \gls{QVTR} constructs.
If this were the case, constraints would have been missed, which could have resulted in identifying consistency relations as compatible although they are not.
Thus, as an answer to \autoref{eq:compatibility:correctness}, the results indicate that we can expect the analysis to operate correctly.


\subsubsection{Applicability}

\mnote{Reasons for conservative behavior}
We have discussed that applicability of the approach especially depends on how often it fails in terms of not proving compatibility although the given consistency relations are compatible.
In particular, conservative behavior of the approach can occur for the following two reasons.
\begin{properdescription}
    \item[Redundancy Notion:] Compatibility of consistency relations is proven by identifying relations that follow the definition of left-equal redundancy (see~\autoref{def:leftequalredundancy}). Since this redundancy notion is not the weakest one that is compatibility-preserving, it may be a too strong requirement for identifying compatibility-preserving consistency relations.
    \item[Redundancy Undecidability:] \autoref{def:consistencyrelation} for consistency relations relies on an extensional specification of consistency, which enumerates usually infinite sets of elements.
    Since such sets cannot be compared programmatically, our practical approach relies on intensional specifications in \gls{OCL} as used by \gls{QVTR}, which describe how consistent element pairs can be derived.
    \gls{OCL} is, however, in general undecidable, because it can be transformed into first-order logic~\cite{beckert2002ocltranslation}.
\end{properdescription}

\mnote{OCL limitations}
In particular, the higher the number of quantifiers within a formula, the more likely its satisfiability will be undecidable.
Since variables in consistency relations are translated to existentially quantified formulae, the number of variables in a consistency relation is crucial for deciding satisfiability.
Not all available \gls{OCL} constructs may be necessary to describe relevant consistency relations, still constructs involving operations on collections, which are transformed into quantified formulae, and strings are especially problematic.
For example, $\mathvariable{toUpper}$ and $\mathvariable{toLower}$, which we have also used in our running example, cannot be transformed into formulae for state-of-the-art \gls{SMT} solvers like Z3 and thus cannot be considered for detecting redundancies.
Additionally, \gls{SMT} solvers use heuristics, which prevents a systematic evaluation of the kinds of consistency relations that can be analyzed by the approach.

\mnote{Metric evaluation shows high recall}
According to the results in \autoref{tab:correctness_evaluation:compatibility_results} from applying our prototypical implementation to the scenarios introduced in \autoref{tab:correctness_evaluation:compatibility_scenarios}, consistency relations were correctly classified as compatible in twelve out of the 15 scenarios, whereas the implementation was not able to prove compatibility in the remaining three scenarios, thus delivering three false negatives.
This leads to a recall value of \SI{80}{\percent}.
\begin{align*}
    &
    \mathvariable{recall} = \frac{\mathtextspacearound{true positives}}{\mathtextspacearound{true positives + false negatives}} = \frac{12}{12+3} = 0.8
\end{align*}
This is a first indicator for high applicability of the approach, as it could prove compatibility in most of the cases in which the relations were compatible.

\mnote{Reasons for false negatives}
The Scenarios $8$, $18$, and $19$ introduced in \autoref{tab:correctness_evaluation:compatibility_scenarios} were not identified as compatible although they actually are.
In all cases, the \gls{SMT} solver should have returned \emph{unsatisfiable} but instead returned \emph{unknown}.
In each scenario an actually redundant consistency relation was not removed, thus not identifying the relations as compatible.
In detail, in Scenario $8$ a precondition ensures that an element is included in the intersection of two set literals, but the solver was not able to check that properly.
In Scenario $18$, the transitive inclusion of sets was defined, and in Scenario $19$, role names of classes with equivalent identifiers were considered, which the solver was both not able to check properly as well.
In summary, all observed false negatives were caused by undecidability of satisfiability of the first-order formulae that were derived from the \gls{OCL} constructs.

\mnote{Answer to evaluation question}
In conclusion, the evaluation has shown that basic operations on primitive data types, even with non-trivial constraints involving integer equations and string operations, were treated correctly.
This led to a success rate of \SI{80}{\percent}.
As an answer to \autoref{eq:compatibility:applicability}, the approach was unable to prove compatibility in only \SI{20}{\percent} of the cases, in which more complex operations and structures requiring many quantifiers were involved, for which satisfiability could not be proven by the used \gls{SMT} solver.
Most importantly, however, this limitation only concerns the chosen \gls{SMT} solver approach but neither the general concept of the formal framework and approach nor the practical realization itself.
In particular, we did not find a scenario in which our redundancy notion was too strict for proving compatibility.
Using a different \gls{SMT} solver or, more generally, even a different approach to validate redundancy of consistency relations can even improve the applicability results.


\subsection{Discussion and Validity}

\mnote{Result summary}
The evaluation of our compatibility analysis approach has shown that in the scenarios considered in the case study it operates correctly and shows a low degree of conservativeness, i.e., it is able to validate compatibility in most cases.
This indicates correctness and high applicability of the approach.
Still, there are some threats to the validity of these results, which we discuss after general conclusions on the benefits of the proposed approach.

\subsubsection{Benefits}

\mnote{Full automation}
In general, the approach is supposed to support transformation developers in designing transformation networks by checking compatibility of transformations during their individual development or when combining them to a network.
With the example depicted in \autoref{fig:compatibility:unwanted_behavior}, we have shown that incompatible consistency relations can prevent the transformations from finding consistent models.
Thus, incompatibilities eventually lead to failing executions of transformation networks, which, in turn, require transformation developers to find the reasons for that.
Our approach provides a benefit by preventing such issues or at least by supporting the developer in finding their causes when running the analysis after a failure occurs.
Due to its full automation, it requires no further effort than running the analysis.
Additionally, a manual process of ensuring compatibility or finding incompatibilities requires manual alignment of transformations or the definition of test cases, which only validate but do not verify compatibility.
Thus, such manual techniques can only make existentially quantified statements about the existence of incompatibilities, whereas our approach makes universally quantified statements about their absence.

\mnote{Degree of conservativeness}
Finally, even if the proposed approach had a high degree of conservativeness, i.e., if it produced a higher number of false negatives in other scenarios than in our evaluation, the approach still provides benefits.
First, the approach would still be able to prove compatibility at least in few cases.
Second, even if the approach cannot prove compatibility, it may at least detect some redundant relations and thus reduces the effort for the transformation developer to find incompatible relations.
It would even be possible to define an interactive approach in which the removal of redundant relations by proof and by user decision is combined, which we propose as future work in the subsequent section.
In such a process, the user could be asked to manually declare redundant relations when the automated approach does not find further ones.
Afterwards, the automated approach can proceed.


\subsubsection{Threats to Validity}

\mnote{Limitations of external validity}
We have designed the evaluation carefully, such that it gives appropriate insights regarding correctness and applicability of the approach.
Still, due to limited complexity of the considered scenarios, threats especially regarding external validity of the results exist.

\mnote{Scenario selection}
The evaluation scenarios of the case study were developed specifically for the evaluation of the approach.
Thus, they may potentially not sufficiently represent actual transformation networks.
On the other hand, the scenarios were designed to test different aspects of the approach and thus represent an extensive set of consistency relations and also consider edge cases.
Scenarios not developed for the evaluation may not or only rarely cover specific and edge cases.
In fact, most meaningful results could potentially be achieved with a combination of externally developed scenarios and evaluation-specific scenarios.
However, the limited availability of scenarios, especially of scenarios developed with the tools we have used for the prototype and contain incompatibilities, prevents this.

\mnote{Scenario complexity}
The defined scenarios only contain \gls{OCL} constructs that the approach currently supports.
Thus, unsupported constructs are not covered by the evaluation, which may be a bias.
The algorithm would, however, not yield a result in such scenarios anyway, thus this would not give further insights.
Additionally, this is only a limitation of the implementation and not a conceptual limitation of the approach.
The actual threat is that more complex relations, which are currently not supported by the implementation, may not be covered by our definition of redundancy.
That would be an actual limitation also of the formal approach.
In consequence, this has to be further evaluated in subsequent evaluations.

\mnote{Scenario size}
The considered scenarios only contain up to four metamodels with pairwise consistency relations.
Actual transformation networks will probably contain more and larger metamodels and consistency relations.
This is, however, not a threat to validity regarding correctness, because the inductive definition of the approach makes it independent from the number of metamodels and relations to consider.
It may only affect applicability, as increasing size may lead to logic formulae which the \gls{SMT} solver is not able to resolve.
The size of scenarios may especially affect the performance and scalability of the approach, which we did not analyze in our evaluation and discuss in the subsequent limitations.

\mnote{Conclusion of threats}
In consequence, our evaluation gives an initial indicator for the correctness and applicability of our approach based on well-selected evaluation scenarios but is potentially restricted in external validity due to the limited set and complexity of scenarios.
To improve evidence in external validity, applying the approach to further and larger transformation networks would be beneficial.
However, acquiring such networks is difficult.
Especially, transformations in existing networks can be expected to be aligned with each other, thus not containing incompatibilities and limiting the evaluation to positive cases.
A possibility to reduce that problem would be the manual extension of such networks by adding transformations with redundant or incompatible consistency relations.
This would directly deliver a ground truth against which the results of the approach on these modified networks can be validated.

\subsection{Limitations and Future Work}

\mnote{Implementation and evaluation limitations}
We discuss two types of limitations of our approach.
First, we consider limitations of the current state of implementation.
Second, we discuss limitations of the current state of evaluation, which may have masked limitations of the current concept.
In addition, we discuss the opportunities for future work that these limitations as well as the conceptual core of the idea to prove compatibility and processes to use it provide.

\paragraph{Practical Approach Realization}
\mnote{Technical realization limitations}
The proposed practical approach for \gls{QVTR} has fundamental as well as technical limitations.
First, \gls{SMT} solvers are limited such that they cannot analyze all kinds of formulae regarding satisfiability.
Thus, even if we can transform all kinds of \gls{QVTR} and \gls{OCL} constructs into logic formulae, they cannot necessarily be checked for satisfiability, as we have shown in the applicability evaluation.
Second, we do not yet support all kinds of \gls{QVTR} relations, as we do not yet provide a transformation for all kinds of \gls{OCL} constructs into logic formulae.
This is, however, only a technical limitation that can be solved by additional implementation effort.

\mnote{Approach completion and operationalization alternatives}
In future work, we will thus extend the operations for which translations to logic formulae are defined, so that we can apply the approach to more sophisticated case studies.
This will provide further indicators for the general applicability of the approach.
%
%\mnote{Operationalization alternatives}
In addition, we will consider alternative realizations of the approach that circumvent the limitations of \gls{SMT} solvers in general.
The limitation of cases that a theorem prover can analyze can restrict applicability of our approach, and in the scenarios considered in our evaluation in \autoref{chap:correctness_evaluation:compatibility}, it was even the only limitation regarding applicability.
To circumvent or mitigate this limitation, it is possible to implement the approach in \autoref{chap:compatibility:practical_approach} by means of other formal methods. 
For example, interactive theorem provers can potentially prove redundancy of consistency relations in more cases. 
Another possibility is the use of multiple formal methods next to \gls{SMT} solvers, as some formal methods can provide proofs in cases in which others can not.
Although this improves the effort for developing the translations, the simultaneous use of different symbolic computation tools can increase the chance of finding redundancy proofs.
Additionally, it may even be beneficial to simplify the \gls{OCL} statements transformed into logic formulae where possible, like discussed by \textcite{cuadrado2019OclOptimization-SoSym}.
On the one hand, this can improve the chance of success of the \gls{SMT} solver.
On the other hand, it can make it easier for a transformation developer to understand the reasons why the algorithm failed if the checked expressions are simpler.

\paragraph{Benefits Evaluation and Development Process}
\mnote{Evaluation limitations}
We have not provided an evaluation for the benefits that we claim for our approach.
First, to the best of our knowledge, there are no competitive approaches to compare our one with.
Second, it automates a manual process without requiring additional effort, thus compared to the baseline of performing the process manually, it provides an inherent and essential benefit.
Thus, further empirical evaluation in a user study could only provide a quantitative measure of the benefits rather than the qualitative one we gave by argumentation.
Such an evaluation could especially consider a development process in which the approach is used and evaluate whether that whole process improves by using our approach.

\mnote{Interactive process}
Such a process specification and evaluation should be part of future work.
Our approach is only able to prove compatibility but not to prove incompatibility. If the approach does not identify a network as compatible, it may be incompatible or not.
For that reason, we aim to define a holistic process for applying the approach, which integrates further information given by the user into the process of proving compatibility.
Since the approach operates inductively, it can simply allow the transformation developer to perform single induction steps.
If the algorithm is not able to prove compatibility, i.e., if it does not find further redundant relations, it can present the network, in which the algorithm already removed some redundant relations, to the transformation developer.
He or she is then asked to declare a cycle of consistency relations as compatible, for which the algorithm is not able to prove it or which are even not compatible intentionally.
Afterwards, the algorithm could proceed with finding further redundant relations to prove compatibility, based on the decision of the user.
As a result, the approach would be applicable to more scenarios in which compatibility is intentionally not given or in which the algorithm on its own is not able to prove it.

\paragraph{Compatibility Notion and its Effects}
\mnote{Optimality of compatibility notion}
The notion of compatibility was derived from the goal of finding contradictory consistency relations that can prevent transformations from finding consistent models after changes.
Additionally, it prevents the specification of contradictory and thus unintended consistency relations.
Although we have shown at examples that our notion of compatibility fulfills both these notions, it is unclear whether this notion is kind of optimal in the sense that there exists no other notion that covers even more unwanted cases.

\mnote{Effects on orchestration problem}
Evaluating the central purpose of the approach to improve the ability of transformations to find consistent models, i.e., to improve dealing with the orchestration problem, is part of our future work.
In fact, compatibility ensures that the ability of not finding a consistent orchestration due to the orchestration problem decreases, thus reducing the ability that transformation networks fail or do not terminate.
While we have shown this at examples in this work, we will empirically evaluate in future work how compatibility affects the ability of transformation networks to find consistent models and, if possible, even formally prove and analyze that effect. 

\paragraph{Relaxation of Redundancy Notion}
\mnote{Optimality of redundancy notion}
We have defined the notion of left-equal redundancy (see \autoref{def:leftequalredundancy}), which is proven to be compatibility-preserving.
It is, however, unclear whether a more relaxed notion of redundancy exists that is still compatibility-preserving.
Our implementation follows an even stricter notion of redundancy and still no limitations of applicability occurred in the case study.
If, however, other case studies reveal the necessity of a weaker redundancy notion to be able to prove compatibility in more cases, either the notion used in the implementation needs to be relaxed or even the formal foundation needs to be adapted.
Thus, we still aim to find the weakest possible notion of redundancy that is still compatibility-preserving, if it exists, in future work.
This especially involves finding scenarios in which our notion of left-equal redundancy is too restrictive.

\paragraph{Performance and Scalability}
\mnote{Performance and scalability evaluation}
We have neither measured nor formally evaluated the performance and scalability of our approach and especially its practical realization.
Applicability may be affected if the approach required too much time to be executed.
\gls{SMT} solvers, such as the used Z3 solver, depend on heuristics, which makes their performance hardly predictable.
Thus, it would be important to evaluate performance of the approach in a case study.
In our case study, we did not observe any time-consuming scenarios.
However, transformation networks with more and larger transformations and especially many cycles of consistency relations need to be investigated to make generalizable statements on the performance and especially the scalability of the approach.
Since the approach is applied as an offline analysis, which does not require instant feedback, it does not have to fulfill real-time requirements.
Results should, however, still be delivered in an acceptable amount of time to achieve acceptance of the approach.

