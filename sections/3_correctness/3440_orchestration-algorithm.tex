\section{A Conservative Application Algorithm}
\label{chap:orchestration:algorithm}

\mnote{Always terminating, correct algorithm}
We have argued why it is inevitable that any algorithm realizing an application function cannot be optimal and thus will not be able to find a consistent orchestration although it exists and, in that case, either return $\bot$ or not even terminate at all.
Apart from minor improvements, such as the avoidance or detection of alternations, to improve the probability to find a consistent orchestration, or general strategies like backtracking for trying different orchestrations, we did not find systematic ways to improve optimality of the application function.
Nevertheless, we want to find an algorithm that is at least correct and does always terminate, even if it does not implement a systematic way to improve optimality.
Thus it operates conservatively.

\mnote{Artificial execution bound drawback}
It is possible that \autoref{algo:orchestration:application} does not terminate, because it generates an infinitely long orchestration, thus never leaving the loop in Lines~\ref{algo:orchestration:application:line:startorchestrate}--\ref{algo:orchestration:application:line:endorchestrate}.
To ensure termination we need to introduce an upper bound for the number of executed transformations.
We have shown in \autoref{theorem:orchestration_fixed} that no natural upper bound exists, thus even the shortest consistent orchestration for specific inputs can be arbitrarily long. %(see example in \autoref{fig:orchestration:no_upper_bound}).
Any arbitrary bound can prevent the algorithm from finding consistent orchestrations.

\mnote{High number of executed transformation unwanted}
From an engineers perspective, we may, however, consider the behavior that an arbitrary high number of transformation executions is required to yield consistent models as unwanted.
Although the examples we have given are valid, they are rather artificial.
We claim that a transformation network that requires a rather high number of executions compared to the number of contained transformations to find consistent models does not operate as expected.
In particular, if such a high number of executions is required to find a consistent orchestration, it will be difficult to identify the reason for not finding a consistent execution in case the algorithm returns $\bot$.
Thus, we introduce an artificial upper bound for the number of transformation executions.
That bound will be well-defined, such that we can reasonably assume that no more executions are practically necessary.

\mnote{Design goals of orchestration}
In the following, we propose design goals for a conservative application algorithm and the so called \emph{provenance algorithm}, and finally prove its correctness and termination properties.
The algorithm was developed together with Joshua Gleitze in a scientific internship~\owncite{gleitze2021orchestration-FASE}.

% FROM MODELS:
% Finally, one could question whether it is relevant if an execution strategy can be guaranteed to terminate. Execution strategies will be used to tell users whether changes they made can be incorporated into the other models automatically. In consequence, users should reliably and timely get a response. We might compare this situation to merging changes in version control systems. There, users also want a reliable and timely response on whether their changes could be incorporated automatically, or whether they need to resolve conflicts manually

% From an engineering perspective, unbounded numbers of executions is still unwanted behavior. We claim that a transformation network that takes thousands of executions of the same transformation to find a consistent state works not as expected and if running into a failure would expose severe problems to find the reasons for that failures.
% Thus, we propose to simply abort the execution after some time to be sure not to run in an endless loop.

% Finally, this problem is comparable to ordinary programming, because there the same situations regarding alternation and divergence can occur that result in non-termination of a program.
% As we all know, it is impossible to systematically avoid that, but just possible to carefully develop the program and apply best practices to avoid such situations.

% \textbf{Conclusion:} Apart from minor discussed improvements, we cannot systematically improve optimality / reduce conservativeness and also not dynamically terminate when divergence is detected. Especially, there is no guarantee wo when also alternation is detected. There can be an arbitrary high number of execution before alternation is detected.
% Thus, we propose an algorithm that terminates deterministically and returns $\bot$ also when it is not able to find a solution in a fixed number of steps, but improves the ability for the transformation developer or user to find out why in specific situation no solution can be found.

% \todo{Question whether having no upper bound is only a theoretical or also a practical problem. The given example is rather artificial, so we may stop in practice without excluding relevant cases}

\subsection{Design Goals}
\label{chap:orchestration:algorithm:goals}

\mnote{Degrees of freedom}
An adapted version of \autoref{algo:orchestration:application} that always terminates has two degrees of freedom.
First, the execution order of transformations needs to be determined by defining the function \function{Orchestrate}.
Second, an upper bound for the number of executions of transformations, thus the number of loop executions in Lines~\ref{algo:orchestration:application:line:startorchestrate}--\ref{algo:orchestration:application:line:endorchestrate}, needs to be defined.

\mnote{Importance of execution order}
We have discussed that improving optimality is not an achievable goal when determining the transformation execution order by the \function{Orchestrate} function.
Since we know that the algorithm will always produce false negatives, i.e., it will not find a consistent orchestration although it exists, it is important for a transformation developer or user to be able to identify the reasons for that in practice.
The algorithm can support them in this regard by delivering the final state of the models when the orchestration aborted.
The execution order that was chosen until that state was reached is of central importance for identifying the reasons for failing.
Consider that transformations are executed in an arbitrary order and then only some of the models of the final state are actually consistent.
Apart from investigating the complete sequence of executed transformations, there is no clue for the user to find the reasons for the algorithm to fail, thus about \emph{provenance} of the error.
We have introduced this goal as the \emph{comprehensibility} property in \autoref{chap:introduction:consistency:orchestration}.

\mnote{Incremental consistency restoration}
To improve identifying the reason whenever the algorithm fails, we propose the following principle for determining an orchestration:
{
\setlength{\leftmargini}{2em}
\begin{quote}
    \enquote{Ensure consistency among the transformations that have already been executed before executing a transformation that has not been executed yet.}~\owncite{gleitze2021orchestration-FASE}
\end{quote}
}
The principle requires that consistency is ensured incrementally for subsets of the transformations and thus the models.
As long as the models are not consistent to all already executed transformations, only those transformations instead of new ones may be executed until the models are consistent to all of them.
This ensures that consistency is preserved after each change in an incremental way, iteratively improving the number of models and transformations for which consistency is restored.

\mnote{Principle benefits}
This approach improves identifying provenance of a failure of the algorithm, because it restricts the potentially causal transformations to consider.
If the algorithm fails after executing a subset of the transformations $\transformationset{T}_\mathvariable{exec} \subseteq \transformationset{T}$,
then there is some transformation $\transformation{t} \in \transformationset{T}_\mathvariable{exec}$ that is the last of those transformation that was executed for its first time.
Thus, the algorithm found an orchestration of $\transformationset{T}_\mathvariable{exec} \setminus \transformation{t}$ such that the models were consistent to all those transformation, but it was not able to execute $\transformation{t}$ and the transformations in $\transformationset{T}_\mathvariable{exec}$ afterwards, such that the models become consistent to all these transformations.
This helps the transformation developer or user to understand and find the reason for failing in different ways.
First, he or she can ignore any transformation in $\transformationset{T} \setminus \transformationset{T}_\mathvariable{exec}$, as the algorithm already failed to preserve consistency according to the other transformations, which can significantly reduce the number of transformations to consider.
Second, the realization of $\transformation{t}$ is somehow conflicting with the other transformations in $\transformationset{T}_\mathvariable{exec}$. This does not necessarily mean that there is something wrong with $\transformation{t}$, but only that also considering this transformation either induces the situation that no consistent orchestration exists anymore or that it cannot be found.
Third, having a state of the models that is consistent to $\transformationset{T}_\mathvariable{exec} \setminus \transformation{t}$ can be used as a starting point to either identify the occurring problem or to manually restore consistency of the models.

\mnote{Principle not improving optimality}
If the algorithm operates according to the introduced principle and is not able to preserve consistency anymore after it considers an additional transformation $\transformation{t}$, the selected execution order provides the discussed benefits for identifying the reasons for failing.
There may, however, be another orchestration that is able to ensure consistency to $\transformationset{T}_\mathvariable{exec}$. Executing $\transformation{t}$ earlier or integrating further transformations in $\transformationset{T}$ before ensuring consistency to all transformations in $\transformationset{T}_\mathvariable{exec}$ can, of course, result in the algorithm finding a consistent orchestration.
This can reduce optimality of the realized orchestration function, but we claim the discussed benefits to outweigh that.

\mnote{Transformations reacting to others}
We have shown that there is no inherent upper bound for the necessary number of transformation executions.
Rather than specifying a concrete number, be it fixed or depending on the network size, we derive a reasonable artificial bound for the number of executions from a property that we assume reasonable for possible orchestrations of a transformation set.
The idea of that property is that each transformation should be allowed to react to the execution of each possible sequence of all other transformations.
It should, however, not be necessary that a transformation must be executed again after the other transformations reacted to the execution of that transformation.
Thus, if a transformation was executed after applying the other transformations in any possible order, we expect the models to be consistent to that transformation.

\begin{definition}[Reactive Converging Transformations]
    \label{def:reactiveconverging}
    A set of synchronizing transformations $\transformationset{T}$ is \emph{reactive converging} with respect to models $\modeltuple{m}$ and changes $\changetuple{\metamodeltuple{M}}$ if every orchestration of every subset $\transformationset{T}_p \subseteq \transformationset{T}$ in which a transformation $\transformation{t} \in \transformationset{T}_P$ has been executed after a sequence of transformations in $\transformationset{T}_p$ that contains each permutation of those transformations as a (not necessarily continuous) subsequence yields models that are consistent to $\transformation{t}$.
\end{definition}

\mnote{Reactive converging transformations example}
The property does not require that the other transformations were executed in each order consecutively, but only that the orchestration contains each permutation of those transformations, but potentially with other transformations in between.
As an example, assume a set of transformations $\setted{\transformation{t}_{1}, \transformation{t}_{2}, \transformation{t}_{3}}$, which is reactive converging for some input of models and changes.
After executing them for these models and changes in the order $\sequenced{\transformation{t}_{1}, \transformation{t}_{2}, \transformation{t}_{3}, \transformation{t}_{1}, \transformation{t}_{2}, \transformation{t}_{3}}$, the models yielded by that orchestration may still be inconsistent to $\transformation{t}_{1}$, because it was not executed after the order of the transformations $\sequenced{\transformation{t}_{3}, \transformation{t}_{2}}$.
After executing $\transformation{t}_{1}$ once more, the orchestration must yield consistent models, because $\transformation{t}_{1}$ was executed after the two orders of the other transformations $\sequenced{\transformation{t}_{2}, \transformation{t}_{3}}$ and $\sequenced{\transformation{t}_{3}, \transformation{t}_{2}}$.
Likewise, $\transformation{t}_{2}$ was executed after $\sequenced{\transformation{t}_{1}, \transformation{t}_{3}}$ and $\sequenced{\transformation{t}_{3}, \transformation{t}_{1}}$, and $\transformation{t}_{3}$ was executed after $\sequenced{\transformation{t}_{1}, \transformation{t}_{2}}$ and $\sequenced{\transformation{t}_{2}, \transformation{t}_{1}}$.


\subsection{The Provenance Algorithm}

\begin{algorithm}
    \input{algorithms/correctness/orchestration/provenance.tex}
	\caption[Provenance application algorithm]{The provenance algorithm. Adapted from~\owncite[Alg.~1]{gleitze2021orchestration-FASE}.}
	\label{algo:orchestration:provenance}
\end{algorithm}

\mnote{Recursive orchestration algorithm}
We propose an algorithm that realizes the discussed design goal with the function \function{ProvenanceApply} in \autoref{algo:orchestration:provenance}.
The algorithm is a derivation of the general algorithm implementing an application function depicted in \autoref{algo:orchestration:application}.
It first checks for consistency of the given models as a prerequisite for executing the transformations.
Then the algorithm calls the recursive function \function{Propagate}, which implements the orchestration of transformations and returns a change tuple that is yielded by the determined orchestration, which delivers consistent models if applied to the input models.
While this behavior is equal to the one in \autoref{algo:orchestration:application}, the orchestration itself is implemented differently in a recursive rather than an iterative manner, which implicitly ensures termination.

\mnote{Incremental consistency restoration principle}
The function \function{Propagate} implementing the orchestration in a recursive manner acts as follows.
It selects one of the transformations as a candidate to execute next.
This selection ensures that a transformation is selected whose models are affected by any already performed change, such that the transformation may need to perform changes.
Models are affected by a change if any of the two changes in $\change{\metamodeltuple{M}}$ for either of the models that are kept consistent by the selected transformation is not the identity function $\identitychange$.
It then applies the transformation using the generalization function $\generalizationfunction{}$.
If the selected transformation is not defined for the given models and changes, the function may return $\bot$, so that the complete algorithm terminates with $\bot$.
Afterwards, it recursively executes the function \function{Propagate} with the subnetwork given by the transformations that have already been executed and are stored in $\provalgexecuted$.
After that recursive execution, the selected transformation is executed again, and it is checked whether the models yielded by the resulting changes are still consistent to the executed transformations.
If this consistency check fails, the transformations do not fulfill the definition of being reactive converging according to \autoref{def:reactiveconverging}, as we prove later.
If the models are consistent to the transformation, the next candidate is picked.
In effect, the strategy realizes the defined principle in a recursive manner, because after executing a new transformation, the recursive execution ensures consistency to all already executed transformations by applying all already executed transformations again.

\begin{figure}
    \centering
    \input{figures/correctness/orchestration/provenance_example.tex}
    \caption[Exemplary execution of the provenance algorithm]{%
    Exemplary execution of the provenance algorithm for a change in the topmost model. 
    The transformations present to the current execution of \function{Propagate}, as well as the executed and candidate transformations $\provalgexecuted$ and $\provalgcandidate$ are depicted for each iteration (horizontal) and recursion step (vertical). Taken from~\owncite[Fig.~4]{gleitze2021orchestration-FASE}.
}
    \label{fig:orchestration:provenance_example}
\end{figure}

\mnote{Exemplary algorithm execution}
\autoref{fig:orchestration:provenance_example} depicts an exemplary execution of the \function{ProvenanceApply} algorithm for a set of four transformations between four metamodels.
We assume that the algorithm receives four initially consistent models and a change to the topmost one.
The example shows that in each recursion step only the subnetwork of the already executed transformations in $\provalgexecuted$ is considered.
Thus, the set of transformations becomes smaller in each recursive call of \function{ProvenanceApply}.


\subsection{Correctness, Termination and Goal Fulfillment}

\mnote{Properties fulfillment}
The provenance algorithm was intended to implement a correct application function and to always terminate.
Additionally, it is supposed to deliver consistent models whenever the given transformations fulfill \autoref{def:reactiveconverging} for being reactive converging.
In the following, we prove that the algorithm actually fulfills these properties.

\mnote{Termination and correctness}
First, it is easy to see that the algorithm always terminates and always either returns consistent models yielded by an orchestration of the given transformations or $\bot$, which realizes a correct application function according to \autoref{def:applicationfunction} and \autoref{def:applicationfunctioncorrectness}.

\begin{theorem}[Provenance Algorithm Termination]
    \autoref{algo:orchestration:provenance} terminates for every possible input.
\end{theorem}
\begin{proof}
    The algorithm terminates if \function{CheckConsistency}, $\generalizationfunction{}$ and \function{Propagate} terminate.
    We assume termination for the external function \function{CheckConsistency}, because it only validates consistency of the given models.
    \function{Propagate} contains a loop with a recursive call and the external calls of \function{CheckConsistency} as well as $\generalizationfunction{}$.
    Since \function{CheckConsistency} and $\generalizationfunction{}$ terminate, it may only be non-terminating because of the loop in Line~\ref{algo:orchestration:provenance:line:loop_start} and the recursive call in Line~\ref{algo:orchestration:provenance:line:recursive_call}.
    The number of loop executions is limited by the number of given transformations, i.e., $\abs{\transformationset{T}}$, as each iteration selects another transformation and adds it to $\provalgexecuted$.
    Thus, after selecting each transformation once, all transformations are in $\provalgexecuted$ and thus the loop condition is not fulfilled.
    The recursive call receives a set of transformations that is at least one element smaller than the set of transformations given to the calling method, because if $\provalgexecuted = \transformationset{T}$ the loop condition is not fulfilled. If the given set of transformations is empty, the loop is not entered and thus no recursive call is performed. Thus, the recursion depth never exceeds $\abs{\transformationset{T}}$.
\end{proof}

\begin{theorem}[Provenance Algorithm Correctness] \label{theorem:provenance_correctness}
    \autoref{algo:orchestration:provenance} realizes a correct application function. % according to \autoref{def:applicationfunctioncorrectness}.
\end{theorem}
\begin{proof}
    The algorithm receives models and changes to them and it returns models being instances of the same metamodels, thus it fulfills the signature of an application function.
    Additionally, if it returns models, they are the result of a consecutive application of transformations in $\transformationset{T}$, as \function{Propagate} calculates the changes that are applied to the input models to calculate the result by a repeated application of the generalization function $\generalizationfunction{}$ to transformations in $\transformationset{T}$.
    Thus, \function{Propagate} implicitly implements an orchestration function according to \autoref{def:orchestrationfunction} and applies the transformations in the determined order to calculate the result delivered by \function{ProvenanceApply}.
    Thus, \function{ProvenanceApply} fulfills \autoref{def:applicationfunction} for an application function.

    Let us assume that \autoref{algo:orchestration:provenance} does not realize a correct application function.
    \function{ProvenanceApply} may return $\bot$ in \autoref{algo:orchestration:provenance:line:bot_input} or \autoref{algo:orchestration:provenance:line:bot_orchestration}, or it may return models in \autoref{algo:orchestration:provenance:line:return_result}.
    Correctness requires the function to either return $\bot$ or consistent models, which may only be violated by \function{ProvenanceApply} returning inconsistent models.
    This means that for some input models and changes, \function{ProvenanceApply} returns models $\modeltuple{m}_\mathvariable{res} \equalsperdefinition \changetuple{\metamodeltuple{M},\mathvariable{res}}(\modeltuple{m})$, such that there is a transformation $\transformation{t} \in \transformationset{T}$ to which $\modeltuple{m}_\mathvariable{res}$, or more specifically two contained models $\model{m}{i}$ and $\model{m}{k}$, whose metamodels are related by $\transformation{t}$, are not consistent.
    We distinguish three cases:
    \begin{longenumerate}
        \item $\transformation{t}$ was never executed by \function{Propagate}. This means that the changes $\change{\metamodel{M}{i}}$ and $\change{\metamodel{M}{k}}$ in $\change{\metamodeltuple{M}}$ of the two models that are kept consistent by $\transformation{t}$ were always empty, i.e., $\identitychange$, because otherwise $\transformation{t}$ would have been selected in the loop header. Since the initial models $\model{m}{i}$ and $\model{m}{k}$ were consistent to $\transformation{t}$, the returned models are still consistent, because only the identity function is applied to them.
        \item $\transformation{t}$ was executed producing changes $\change{\metamodel{M}{i}}$ and $\change{\metamodel{M}{k}}$, and no other transformation that affects $\model{m}{i}$ or $\model{m}{k}$ was executed afterwards. Then the returned models, i.e., $\change{\metamodel{M}{i}}(\model{m}{i})$ and $\change{\metamodel{M}{k}}(\model{m}{k})$ are consistent by definition of correctness for $\transformation{t}$.
        \item $\transformation{t}$ was executed, and another transformation $\transformation{t}' \in \transformationset{T}$ that involves $\model{m}{i}$ or $\model{m}{k}$ was executed afterwards.
        Since $\transformation{t}'$ was executed after $\transformation{t}$, $\transformation{t}$ was in $\provalgexecuted$ when $\transformation{t}'$ was the candidate $\provalgcandidate$.
        After executing the transformations in $\provalgexecuted$, the candidate $\transformation{t}'$ is applied again in Line~\ref{algo:orchestration:provenance:line:second_execution}. 
        Additionally, consistency to all transformations in $\provalgexecuted$ is ensured by the check in Line~\ref{algo:orchestration:provenance:line:bot_failcheck} after returning from the recursion in which $\transformation{t}$ was executed.
        Thus, the returned models are consistent to both $\transformation{t}$ and $\transformation{t}'$.
    \end{longenumerate}
    The third case can be applied inductively if a transformation is followed by multiple transformations that involve the same models. Thus, all cases lead to a contradiction.
\end{proof}

\mnote{Algorithm complexity}
In addition to these essential properties, we can also derive the upper bound for the number of transformation executions by the algorithm.

\begin{theorem}[Provenance Algorithm Complexity]
    \autoref{algo:orchestration:provenance} executes transformations at most $\mathcal{O}(2^{\abs{\transformationset{T}}})$ times.
\end{theorem}
\begin{proof}
	Let $T(m)$ denote the number of transformation executions the algorithm invokes for a set of transformations $\transformationset{T}$ with $m = \abs{\transformationset{T}}$.
	The set $\provalgexecuted$ is initialized to be empty (Line~\ref{algo:orchestration:provenance:line:executed_init}) and grows by one transformation every iteration of the loop (Line~\ref{algo:orchestration:provenance:line:executed_update}).
    It follows that the recursive call in Line~\ref{algo:orchestration:provenance:line:recursive_call} receives a set of transformations that contains one more transformation in each iteration.
    Thus, given $m$ transformations, \function{Propagate} executes each of them in the loop and then makes recursive calls for $0$ to $m-1$ transformations:
    \begin{align*}
        &
    T(m)	=m+\sum_{i=0}^{m-1}T(i)
    	=2+2\,T(m-1)
        =2*(2^{m}-1) \in \mathcal{O}(2^m) \\
        &
    T(0)	=0 \qedhere
	\end{align*}
\end{proof}

\mnote{Design principle fulfillment}
Finally, the algorithm shall implement the principle defined in \autoref{chap:orchestration:algorithm:goals} to ensure consistency among the transformations that have already been executed before executing a transformation that has not been executed yet.

\begin{theorem}[Provenance Algorithm Design Principle]
    \autoref{algo:orchestration:provenance} ensures consistency among the transformations that have already been executed before executing a transformation that has not been executed yet.
\end{theorem}
\begin{proof}
	After the recursive call in Line~\ref{algo:orchestration:provenance:line:recursive_call}, the model tuple yielded by applying the current changes $\changetuple{\metamodeltuple{M},\mathvariable{candidate}}$ to the initial model tuple $\modeltuple{m}$ is consistent to all executed transformations in $\provalgexecuted$ according to \autoref{theorem:provenance_correctness}. % and
%	\Cref{thm:strategy-correct} applies for every recursive execution of the strategy. 
	%no changes to models involved  to an executed transformations are allowed.% after the recursive call in \cref{line:recursive-call}.
%	Hence,
	% \executed is either empty or 
%	the current model assignment is consistent with all transformations in \executed whenever the algorithm executes a new transformation in \cref{line:first-execution}.
\end{proof}	

\mnote{Algorithm optimality}
We have given \autoref{def:reactiveconverging} for the property of a transformation set to be reactive converging.
This property defines that we do not want transformations to be required to react to changes they performed themselves if all other transformations have been executed afterwards, as we assume this to be a reasonable property that induces an upper bound for the number of transformation executions.
We have used that property as a design goal for the proposed algorithm and can now show that the algorithm always returns consistent models when the transformations fulfill that property, which means that the algorithm implements an optimal application function.

\begin{theorem}[Provenance Algorithm Optimality]
    If the transformation set $\transformationset{T}$ passed to \autoref{algo:orchestration:provenance} is reactive converging according to \autoref{def:reactiveconverging} and if the consistency preservation rules of all transformations in $\transformationset{T}$ are total functions, then the algorithm implements an optimal application function.
\end{theorem}
\begin{proof}
    We show that the algorithm does not return $\bot$ when the input models are consistent, thus an orchestration is always found.
    This is even stronger than optimality, because it means that for every input with consistent models a consistent orchestration exists.

    Since optimality allows the algorithm to return $\bot$ when the input models are inconsistent, returning $\bot$ in \autoref{algo:orchestration:provenance:line:bot_input} is valid.
    The algorithm returns $\bot$ in \autoref{algo:orchestration:provenance:line:bot_orchestration} if \function{Propagate} returns $\bot$, thus we show that \function{Propagate} does not return $\bot$.
    \function{Propagate} returns $\bot$ in \autoref{algo:orchestration:provenance:line:bot_first_execution} and \autoref{algo:orchestration:provenance:line:bot_second_execution} if the application of a $\provalgcandidate$ in \autoref{algo:orchestration:provenance:line:first_execution} or \autoref{algo:orchestration:provenance:line:second_execution}, respectively, returns $\bot$, which cannot occur because transformations are total by assumption.
    \function{Propagate} returns $\bot$ in \autoref{algo:orchestration:provenance:line:bot_recursion} if a recursive call returns $\bot$. If the loop in that recursive call is executed, the arguments for not returning $\bot$ apply recursively. If the loop is not executed in the recursion, the input changes are returned, thus not yielding $\bot$.
    
    Finally, \function{Propagate} returns $\bot$ in \autoref{algo:orchestration:provenance:line:bot_nonreactiveconverging} if the models yielded by applying the changes after the recursive call and reapplying $\provalgcandidate$ are not consistent with the already executed transformations in $\provalgexecuted \cup \setted{\provalgcandidate}$.
    Since the transformation set is reactive converging, this can only be the case if not all permutations of the transformations in $\provalgexecuted \cup 
    \setted{\provalgcandidate}$ have been executed yet.
    We first note that applying the $\function{Propagate}$ function to transformations $\transformationset{t}$ with $\abs{\transformationset{t}} = m$, the result after the first $n \leq m$ loop iterations is the same as when executing $\function{Propagate}$ to the $n$ transformations in $\provalgexecuted$ after $n$ loop iterations.
    We thus show that when reaching Line~\ref{algo:orchestration:provenance:line:bot_failcheck} in the last iteration of the loop, i.e., when the algorithm returns consistent models if the check in that line does not fail, every permutation of transformations in $\transformationset{t}$, and thus in $\provalgexecuted \cup \setted{\provalgcandidate}$, has been executed by induction.
    Applying $\function{Propagation}$ to a transformation set with $\abs{\transformationset{t}} = 1$, the statement is trivially true, because the single transformation is executed in Line~\ref{algo:orchestration:provenance:line:first_execution}.
    Let us assume that the statement is true for a transformation set with $\abs{\transformationset{t}} < i$, but that it is not true for a set with $\abs{\transformationset{t}} = i$.
    Since the execution of the first $i-1$ iterations is equal to executing the algorithm on the $i-1$ transformations selected in these iterations, the algorithm cannot return $\bot$ in Line~\ref{algo:orchestration:provenance:line:bot_nonreactiveconverging} in the first $i-1$ iterations by induction assumption.
    Thus, only in the last iteration not all permutations of transformations may have been executed and thus the check in Line~\ref{algo:orchestration:provenance:line:bot_failcheck} may only fail in that last iteration.
    This means that there is a permutation $\sequenced{\transformation{t}_{1}, \dots, \transformation{t}_{i}}$ of the transformations in $\transformationset{t}$ after the last loop iteration in which they have not been executed yet.
    Let $\transformation{t}$ be the candidate $\provalgcandidate$ of the last loop iteration, and let $k$ be the index of $\transformation{t}$ in that sequence, i.e., $\transformation{t} = \transformation{t}_{k}$.
    Then per induction assumption, the sequence $\sequenced{\transformation{t}_{1}, \dots, \transformation{t}_{k-1}}$ has been executed in one of the previous iterations of the loop.
    Afterwards $\transformation{t}$ was executed in Line~\ref{algo:orchestration:provenance:line:first_execution}.
    Then, the sequence $\sequenced{\transformation{t}_{k+1}, \dots, \transformation{t}_{i}}$ has been executed in the recursive call in Line~\ref{algo:orchestration:provenance:line:recursive_call} by induction assumption.
    Since during the last iteration the recursive call is performed with $\provalgexecuted = \transformationset{t} \setminus \setted{\transformation{t}}$ and thus $\abs{\provalgexecuted} = \abs{\transformationset{t}} - 1$, all permutations of transformations in $\provalgexecuted$, including $\sequenced{\transformation{t}_{k+1}, \dots, \transformation{t}_{i}}$, are executed in the recursive call by induction assumption.
    This is a contradiction.

    In consequence, \function{Propagate} and thus \function{ProvenanceApply} does never return $\bot$, except for inconsistent input models. Since we have already proven that the algorithm terminates always and implements a correct application function, this shows that it implements an optimal application function.
\end{proof}

\mnote{Optimality assumptions fulfillment}
Optimality can, however, only be guaranteed under specific conditions.
Apart from the necessity to be reactive converging, the transformations need to be able to handle every input, i.e., every combination of models and changes, as otherwise selecting a transformation may lead to \function{Propagate} returning $\bot$, because the transformation cannot be applied.
In practice, this assumption will usually not be fulfilled.
Nevertheless, it is theoretically possible to define such transformations and, at least, it leads to well-defined conditions for when we can assume the algorithm to realize an optimal orchestration function.

\mnote{Orchestration problem non-existence}
Although this means that under such specific conditions the algorithm is able to decide the orchestration problem, the problem is actually trivially solved in that case, because for every input there is a consistent orchestration, thus the problem is actually non-existent under these assumptions.

\mnote{Well-defined requirements}
Finally, it is an open question how far we can assume sets of transformations to be reactive converging in practice.
We have, however, not introduced this as a property that should be fulfilled by transformations, as it is obviously hard to ensure or even analyze that property.
In fact, it is only supposed to be a well-defined property that allows us to define a reasonable upper bound for the execution of transformations and thus to allow us to define an algorithm that always terminates without using a completely arbitrary upper bound for determining when to terminate.


\subsection{Provenance Identification Improvement}

\mnote{Information about failure state}
We have motivated the provenance algorithm with the idea to improve the ability of a transformation developer or user to find the reason for the algorithm not to yield consistent models for certain inputs.
The proposed \autoref{algo:orchestration:provenance} only returns $\bot$ in those situations and does thus not directly support that process.
The necessary information for improving the identification of provenance for the failure is, however, present in the algorithm and can be easily retrieved.

\mnote{Reasons for algorithm failing}
The algorithm may fail, because it is, at some point, not able to execute a candidate transformation (Line~\ref{algo:orchestration:provenance:line:bot_first_execution} or Line~\ref{algo:orchestration:provenance:line:bot_second_execution}), or because after executing a new transformation consistency to the previously executed transformations cannot be achieved without letting one of the transformations react to the reaction of all other transformations to its own changes (Line~\ref{algo:orchestration:provenance:line:bot_failcheck}), which we defined as the property of reactive convergence.
In that case, we at least know that after the previous loop iteration consistency to all transformations that have been executed so far could be achieved.

\mnote{Relevant failure state information}
Whenever the \function{Propagate} function fails and returns $\bot$, we know that for the current transformations in $\provalgexecuted$ an orchestration exists that yields the current changes in $\changetuple{\metamodeltuple{M}}$, %= \tupled{\change{\metamodel{M}{1}}, \dots, \change{\metamodel{M}{n}}}$
for which we know that when applied to the original models, the result $\changetuple{\metamodeltuple{M}}(\modeltuple{m})$ %\tupled{\change{\metamodel{M}{1}}(\model{m}{1}), \dots, \change{\metamodel{M}{n}}(\model{m}{n})}$
is at least consistent to $\provalgexecuted$.
We also know that the algorithm was not able to ensure consistency to the current candidate transformation $\provalgcandidate$.
This is exactly the information for which we already discussed in \autoref{chap:orchestration:algorithm:goals} the benefits with respect to the underlying design principle of recursively ensuring consistency for subsets of the transformations for the ability to identify the reasons for not finding a consistent orchestration.
Thus, implementing the algorithm such that it also delivers $\provalgcandidate$, $\provalgexecuted$ and the current changes $\changetuple{\metamodeltuple{M}}$ reduces the necessary model states and transformations to consider for a transformation user or developer to identify why no consistent orchestration was found.

%Unfortunately, the algorithm does help in finding whether no orchestration exists at all or whether only the algorithm was not able to find it.
%Maybe map that to inapplicability of transformations to non-existence vs. fail after recursive application to non-finding

\mnote{Improving locality}
The algorithm and the ability to identify reasons for the algorithm to fail may be further improved by determining a reasonable order for the execution of transformations in the loop of the \function{Propagate} function.
The loop at least ensures that no transformations are executed that are not yet affected by any change and thus would not produce changes.
It can, however, also be reasonable to first select transformations for which both models have already been modified before selecting transformations for which only one model has been modified.
This can further improve locality of the changes made until the algorithm fails, because less models may have been modified until the algorithm fails.
We also discuss these benefits as results of the evaluation in \autoref{chap:correctness_evaluation:orchestration}.

%\todo{Discuss that is may make sense to first use transformations between already involved models rather than passing through the complete network.}


