\chapter{Topologies and Properties
    \pgsize{15 p.}
}
\label{chap:properties}

%%% Define what a transformation networks is
\section{Assumptions and Terminology}
\label{chap:properties:terminology}

We shortly clarify our assumptions and introduce a terminology for consistency %and its preservation % based on definitions of models, consistency and consistency preservation 
that we %later 
use to explain our classification.
%Short introduction of transformations (not deeply necessary on ICMT) -- leave out, put to assumptions
%\subsection{Assumptions}
%\label{sec:foundations:assumptions}
%We consider incremental \acp{BX} for preserving consistency between models.
%Furthermore, we 
We assume that consistency of more than two types of models is specified using networks of \acp{BX} rather than multidirectional approaches for two reasons:
First, it is easier to think about binary than about $n$-ary relations~\cite{stevens2017a}.
Second, a domain expert usually only knows about consistency relations within a subset of all model types used to develop a system, so modularizing transformations is inevitable.
It was also the result of a Dagstuhl seminar that \enquote{it seems likely that networks of bidirectional transformations suffice for specifying multidirectional transformations}~\cite[p. 7]{cleve2019dagstuhl}.
Finally, we investigate of a subset of problems that can actually occur, as in a concrete scenario $n$-ary relations may exist that cannot be expressed by sets of binary relations.
Although we limit our considerations to the assumed scenarios, most of our findings could also be extended to a modularization into smaller $n$-ary relations rather than binary relations.

%Furthermore, we focus on consistency preservation rather than only consistency checking.
%Therefore, we follow a \emph{normative} approach, which means that we always assume that a specification of consistency defines when models are consistent rather than having another, maybe information notion of consistency that has to be met and potentially validated.
%
% \begin{itemize}
%     \item Incremental
%     \item Bidirectional
%     \item Delta-based(!)
%     \item Normative
%     \item Binary modularization, with domain experts
% \end{itemize}
%
% \todoHeiko{Make assumptions explicit! (From Intro) Incremental, distributed development (divide and conquer), ...}
% \begin{itemize}
%     \item Repair, not only checking!
%     \item Incremental transformations for consistency preservation
%     \item Independent development of binary transformations by domain experts -> Necessity to independently develop and combine transformations afterwards
%     \item Normative: We define what is consistent (rather than defining consistency and checking it against a "real" relation)
% \end{itemize}
%
% \paragraph{Terminology}
%
% Failure, (Fault), Mistake, Cause - define?
%
%
%\subsection{Terminology}
%
%\subsection{Models}
\label{sec:foundations:models}
%\todoHeiko{Nicht Metamodelle, sondern Model Sets sagen? Diese Mengen sind ja eigentlich keine Metamodelle, sondern werden von einem Metamodell induziert. Oder auch model type?}
%
%We provide short definitions for models and the specification of consistency, on which we build our classification to clearly separate the issues that we investigate.
%Provide a short formalization of what is necessary to later define the different issues, e.g., what do these issues mean in terms of a formal definition. \todoHeiko{do this here in ICMT paper or later in SoSym?}
%\subsubsection{Models and \Metamodels}
%Our definition of models follows the one used by \textcite{stevens2017a}. 

\begin{definition}[Model]
A model $M = \{e_1, e_2, \ldots\}$ is a finite set of not further defined elements, such as objects, attribute and reference values.
\end{definition}

The exact representation of the model contents is not relevant for our work, which is why we use this lightweight definition. 
It allows us to transfer the insights to arbitrary models, such as models that are conform to the \ac{EMOF}~\cite{mof}.

\begin{definition}[Model Type]
A model type $\mathcal{M} = \{M_1, M_2, \dots\}$ is the (usually but not necessarily infinite) set of all models $M_1, M_2, \dots$ that are instances of $\mathcal{M}$.
\end{definition}

%So for a model type $\mathcal{M}$, a model $M$ is considered an instance of $\mathcal{M}$ iff $M \in \mathcal{M}$.
In the following, let a model $M_i$ be always an instance of model type $\mathcal{M}_i$.
This definition constitutes an \emph{extensional description} of models and does not explicitly consider actual instantiation relations between classes and objects, attributes and their values etc., other than containment in the respective model type. 
We also use the term \emph{\metamodel} when referring to an abstract syntax of classes, attributes and associations, as defined in the OCL standard~\cite[A.1]{ocl}. 
A \metamodel constitutes an \emph{intensional description} of models, from which the model type could be derived by enumerating all valid instances, i.e., all models with arbitrary instantiations of classes, their attributes and associations.
%In fact, common definitions of \metamodels require abstract specifications of elements and their relations, which can be instantiated. Our definition rather covers a description of models sets, which is appropriate for our case.

%\subsection{Consistency}
%\label{sec:foundations:consistency}

%In addition to models, we define the basic terms \emph{consistency specification}, % expressing the consistency constraints that have to hold, 
%and \emph{consistency preservation specification}: % , expressing the rules to preserve consistency after changes.

\begin{definition}[Consistency Specification]
\label{def:consistency_specification}
A \emph{consistency specification} $\mathit{CS}$ for model types $\mathcal{M}_1, \ldots, \mathcal{M}_n$ is a relation $\mathit{CS} \subset \mathcal{M}_1 \times \ldots \times \mathcal{M}_n$ between models that are consistent. 
% the tuples of instances of model types $\mathcal{M}_1 \times \ldots \times \mathcal{M}_n$ that are consistent.
We denote a binary consistency specification for model types $\mathcal{M}_i$ and $\mathcal{M}_j$ as $\mathit{CS}_{i,j}$.
\end{definition}

%\todoHeiko{In der Relation zu sein heißt konsistent zu sein. D.h. wenn ich keine Einschränkungen bzgl. Konsistenz mache (alle Modelle sind konsistent zueinadner), enthält die Relation alle möglichen Paare von Modellen}

%This way of defining consistency between models by enumerating consistent instances is comparable to \cite{stevens2017a}.
Enumerating consistent instances to define consistency is comparable to \cite{stevens2017a}.
%Enumerating consistent models is not practically applicable in contrast to constructive approaches that define how to construct consistent models, but it eases expressing properties of consistency. % mathematical statements about consistency.
If there are no restrictions on when models are consistent, %all models are always consistent by definition, and 
$\mathit{CS}$ % the consistency specification 
contains all tuples of models.
We denote restrictions for models to be in $\mathit{CS}$ as \emph{consistency constraints}.
It would, in theory, also be possible to define $\mathit{CS}$ on an infinite number of model types. However, for ease of understanding and because of missing practical examples, we decided to fix the number of model types in a consistency specification.

We primarily consider binary consistency specifications, which are the binary relations that define consistency pairs of models, %, which only specify consistent instances of two model types.
and also binary specifications for consistency preservation, which are functions that restore consistency between two models after one of them was modified. 
% We also consider binary specifications for consistency preservation, which concern the modification of one model and the update of it and a model of another type. 
In the following, we introduce such consistency preservation specifications.
%To simplify the composition of such functions between more than two model types, 
Each consistency preservation specification concerns modifications in instances of two model types.
However, instead of defining such a function on two model types, we define it on an arbitrary number of model types, but restrict modifications to instances of two of them.
In consequence, a set of binary consistency preservation specifications for an arbitrary number of model types can be defined, whose signatures of input and output are all equal.
This leads to a rather verbose definition of consistency preservation specifications, but eases the composition of such functions between more than two model types.
If the function only considered the two involved model types, the composition definition would have to properly consider matching function signatures, whereas our definition allows the composition of all functions with each other.
%Therefore, we define them on an arbitrary number of model types, but restrict modifications to two of them.
A consistency preservation specification expects and returns a tuple of pairs, each representing a change by containing an original and a modified model.
The original models in a tuple are always consistent, but a specification may update the modified models. % may be updated by the specification.

\begin{definition}[Consistency Preservation Specification]
\label{def:consistency_preservation_specification}
% A consistency preservation specification $CPS_{CS}$ is a function for a consistency specification $CS$ that expects a tuple of original models, and one model having a modified state regarding one of the original ones, and maps it to a new set of models:
% \begin{align*}
%     CPS_{CS} : (\mathcal{M}_1), \ldots, \mathcal{M}_n, \mathcal{M}_i) \mapsto (\mathcal{M}_1, \times \ldots \times \mathcal{M}_n), i \in \{1, \ldots, n\}
% \end{align*}
% For a consistency specification $CS_{i, j}$, a \emph{consistency preservation specification} $CPS_{CS_{i.j}}$ is a function between a tuple of model pairs, each containing one original and one modified model of the same model type, and maps it to a new tuple of model pairs.
% \begin{align*}
%     CPS_{CS} : ((\mathcal{M}_1, \mathcal{M}_1), \ldots, (\mathcal{M}_n, \mathcal{M}_n) \mapsto ((\mathcal{M}_1, \mathcal{M}_1), \ldots, (\mathcal{M}_n, \mathcal{M}_n)
% \end{align*}
% so that for $M_1, \ldots, M_n$ with $(M_i, M_j) \in CS_{i,j}$ and modified instances $M'_i, M'_j$:
% \begin{align*}
%     & \forall M'_k \in \mathcal{M}_k, k \in \{1, \dots, n\}\backslash\{i\}:\\
%     & \hspace{1em} ((M_1, M''_1), \ldots, (M_n, M''_n)) = CPS_{CS_{i,j}}((M_1, M'_1), \ldots, (M_n, M'_n)) \\
%     & \hspace{2em} \Rightarrow CS_{i,j}(M''_i, M''_j) \land \forall k \in \{1, \dots, n\}\backslash\{i, j\} : M'_k = M''_k
%     %
% \end{align*}
For a binary consistency specification $\mathit{CS}_{i, j}$, a \emph{consistency preservation specification} $\mathit{CPS}_{\mathit{CS}_{i,j}}$ is a partial function defined if $(M_i, M_j) \in \mathit{CS}_{i,j}$
that maps a tuple of model pairs, each containing an original model $M_k \in \mathcal{M}_k$ and a modified model $M'_k \in \mathcal{M}_k$, to a new tuple of model pairs:
\begin{align*}
    \mathit{CPS}_{\mathit{CS}_{i,j}}: \hspace{0.3em} & \big( (\mathcal{M}_1, \mathcal{M}_1), \ldots, (\mathcal{M}_n, \mathcal{M}_n) \big) \rightarrow \big( (\mathcal{M}_1, \mathcal{M}_1), \ldots, (\mathcal{M}_n, \mathcal{M}_n) \big), \\[0.5em]
    & \hspace{-3.1em} \big( (M_1, M'_1), \ldots, (M_i, M'_i), \ldots, (M_j, M'_j), \ldots, (M_n, M'_n) \big) \\
    & \hspace{-3.1em} \mapsto \begin{cases}
        \big( (M_1, M'_1), \ldots, (M_i, M''_i), \ldots, (M_j, M''_j), \ldots, (M_n, M'_n) \big) & (M_i, M_j) \in \mathit{CS}_{i,j} \\
        \mathit{undefined} & \mathit{otherwise}
    \end{cases}
\end{align*}
so that
\begin{align*}
    (M''_i, M''_j) \in \mathit{CS}_{i,j}
\end{align*}
%holds
%\begin{align*}
    %$(M''_1, \ldots, M''_n) \in CS$
%\end{align*}
% Furthermore, it holds that for models $M_1, \ldots, M_n$ with $(M_1, \ldots, M_n) \in CS$ and a modified instance $M'_i$ of model $M_i$ that 
% \begin{align*}
%     CPS_{CS}(M_1, \ldots, M_n, M'_i) \in CS
% \end{align*}
\end{definition}
%\todoHeiko{Normativ klar machen: Wir definieren, was konsistent ist. Wenn wir CPS angeben, die alle auf leere Modelle abbilden, ist das valide}

\noindent\textit{Remark:} %We assume a normative approach for defining consistency, so it is up to the developer to provide reasonable specifications. 
A specification that always maps to empty models would be valid regarding our definition.
It is up to the developer to provide reasonable specifications. 

%\noindent\textit{Remark:} %We assume a normative approach for defining consistency, so it is up to the developer to provide reasonable specifications. 
%A specification that always maps to empty models would be valid regarding our definition.
%It is up to the developer to provide reasonable specifications. 
%We assume a normative approach for defining consistency, so consistent is what a developer specifies as such. In consequence, a consistency preservation specification that always maps to a tuple of empty models would be valid in our definition.

%Usually, incremental model transformations are used to preserve consistency between models. 

% We consider binary consistency preservation specifications, which concern the modification of one model type and the update of that and one other type. 
% To able to concatenate those specifications, we restrict the number of models appropriately.

%This can be expressed by a consistency preservation specification that is specified on two model types, but to be able to easily concatenate binary consistency preservation specifications on different pairs of model types, we use the following definition that simply restricts the number of modified models appropriately.

% \begin{definition}[Binary Consistency Preservation Specification]
% \label{def:binary_consistency_preservation_specification}
% A binary consistency preservation specification $CPS_{CS{i,j}}$ for a binary consistency specification $CS_{i,j}$ is a function according to \autoref{def:consistency_preservation_specification}, which only changes $\mathcal{M}_i$ and $\mathcal{M}_j$, so that 
% %This means that for models $M_1, \ldots, M_n$ with $(M_1, \ldots, M_n) \in CS$ and modified instances $M'_1, \ldots, M'_n$
% %\begin{align*}
%     %((M_1, M''_1), \dots, (M_n, M''_n)) := CPS_{CS, \mathcal{M}_{i,j}}(M_1, \dots, M_n, M'_i)
% %\end{align*}
% %holds
% %\begin{align*}
%     $CS_{i,j}(M''_i, M''_j) \land \forall k \in \{1, \dots, n\}\backslash\{i, j\} : M'_k = M''_k$
% %\end{align*}
% \end{definition}

We are interested in consistency preservation specifications that can be executed in arbitrary order, so that they finally terminate in a consistent state regarding all consistency specifications, comparable to a fixed-point iteration.
Therefore, it is essential for all specifications to be hippocratic~\cite{stevens2010sosym}, so that no changes are performed when models are already consistent.
Let $\mathcal{CPS}$ be a set of preservation specifications %\mathit{CPS}_1, \dots, \mathit{CPS}_k$ 
for consistency specifications $\mathcal{CS}$. % = \{CS_1, \dots, CS_l\}$, 
We denote the set of consistent model tuples regarding $\mathcal{CS}$ as $\mathfrak{M}_{\mathcal{CS}} = \{(M_1, \dots, M_n) \mid %\forall i, j, 0 \leq i,j \leq n : 
\forall \mathit{CS}_{i,j} \in \mathcal{CS} : (M_i, M_j) \in \mathit{CS}_{i, j}\}$.
We want to achieve that:
\begin{align*}
    & \forall (M_1, \dots, M_n) \in \mathfrak{M}_{\mathcal{CS}} : %\{(M_1, \dots, M_n) \mid \forall CS_{i,j} \in \mathcal{CS} \Rightarrow (M_i, M_j) \in CS_{i, j}\} : \\ %0 \leq i,j \leq n : \exists CS_{i,j} \in \mathcal{CS} \Rightarrow (M_i, M_j) \in CS_{i, j}\} : \\
    % & \hspace{1em} 
    \forall M'_1 \in \mathcal{M}_1, \dots, M'_n \in \mathcal{M}_n : \exists \mathit{CPS}_1, \dots, \mathit{CPS}_k \in \mathcal{CPS} : \\
    %& \forall M_1, \dots, M_n \mid \big( \forall CS_{i,j} \in \mathcal{CS} : (M_i, M_j) \in CS_{i, j} \big) : \\ % (i,j) \mid 1 \leq i, j \leq n
    %& \exists p \in \mathbb{N}: (CPS_1 \circ \dots \circ CPS_k)^p \big( (M_1, M'_1), \dots, (M_n, M'_n) \big) = \big( (M_1, M''_1), \dots, (M_n, M''_n) \big) \\
    & \hspace{1em} \mathit{CPS}_1 \circ \dots \circ \mathit{CPS}_k \big( (M_1, M'_1), \dots, (M_n, M'_n) \big) = \big( (M_1, M''_1), \dots, (M_n, M''_n) \big) \\
    & \hspace{2em} \land \forall \mathit{CS}_{i,j} \in \mathcal{CS} : (M''_i, M''_j) \in \mathit{CS}_{i, j} %\forall (i,j) \mid 1 \leq i, j \leq n : CS_{i, j}(M''_i, M''_j)
\end{align*}

This means that there is always a sequence of consistency preservation specification applications, potentially with multiple applications of the same specification, that ensures that the modified models in all tuples are consistent after applying it.

%\todoHeiko{Zunächst mal die Konkatenierung erklären. Wir nehmen an, dass in einer korrekten Spezifikation eine Konkatenation existiert, die für eine beliebige Änderung wieder ein konsistentes Modell ausspuckt. Dafür ist die Ausführungsreihenfolge der Spezifikationen egal. Da man in der Praxis nicht nur wissen muss, dass die Modelle nach einer ausreichend langen Ausführung der Spezifikationen konsistent sind, sondern auch terminieren muss, wird die hippocraticness Eigenschaft \cite{stevens2007a} vorausgesetzt, nach der Transformationen nichts tun, wenn die Modelle bereits konsistent sind. Cf. Fixpunktiteration} 

Declarative transformation languages are usually well suited to define consistency specifications according to \autoref{def:consistency_specification}, 
from which a consistency preservation specification is %, in the best case automatically, 
derived. 
Imperative transformation languages can be used to define consistency preservation specifications according to \autoref{def:consistency_preservation_specification}. 



\section{Properties of Transformation Networks}

\subsection{Binary Transformation Interoperability}
\label{sec:approach:interoperability}

Multi-model consistency preservation can be a achieved by combining binary transformations to graphs, %of transformations, 
with the transformations being executed transitively.
Since all binary transformations are developed independently of each other, it is necessary that they interoperate properly in a \emph{non-intrusive} way, thus without the necessity for the developer to understand and modify them, which we refer to as \emph{black-box combination}.

Even under the assumption that, in contrast to our introductory motivation, all specifications are free of contradictions, it is easy to see that problems arise when combining binary transformations by transitively executing them.
For example, consider the relations in \autoref{fig:prologue:binary_combination_example}.
If a component is added to the \ac{ADL}, causing a \ac{UML} class creation due to \ref{fig:prologue:binary_combination_example:R1}, which in turn causes a Java class creation due to \ref{fig:prologue:binary_combination_example:R2}, the transformation for relation \ref{fig:prologue:binary_combination_example:R3} does not know that an appropriate class was already created, if the transformations are treated as black boxes.
Consequently, the transformation will create the same class again, which may override the existing one, depending on the implementation and execution order.
A simple solution for this example would be to have all transformations use a common trace model and check for existing elements before creating them in a transformation.
Nevertheless, independently developed transformations will usually not assume that %this only applies if the transformation considers possibly preexisting transformation results, which it will not do in general if it does not assume other transformations to create corresponding elements.
other transformations may already have created corresponding elements.
Additionally, the trace model must allow the transformation engine to retrieve transitive traces.
However, it is unclear if transitive resolution of traces can always be performed, as it can depend on whether the transitive trace belongs the considered consistency relation or another.
%If, in another scenario, the transformation in the example was actually supposed to create an additional class, it would have to ignore the existing trace.

As can be seen in the example, especially the correct handling of trace information in interdependent transformations has to be researched.
This applies not only to element creations, but also other change types, such as attribute or reference changes, especially if they are multi-valued.
In our thesis, we will therefore apply transitively executed binary transformations in different case studies to identify these and potential further problems.
We then want to come up with a catalog of such problems %preventing the black-box combination of transformations 
together with solution patterns for them.
For example, to avoid duplicate element creations, a simple pattern could be to always check for already existing traces for that consistency relation in the transformations.
In consequence, the integration of those patterns into a transformation language or the application of them as a transformation developer is supposed to achieve black-box combinability of the transformations.

\subsection{Challenges and Topology Trade-Offs}
\label{sec:approach:challenges}

Even if independently developed transformations are interoperable, as discussed before, higher-level challenges remain when considering the way in which transformations express relations or occur if compatibility between the transformations is not given.
%Those challenges cannot be solved simultaneously, but induce trace-off decisions that have to be made regarding the way in which transformations are specified. %, as not all of them can be solved simultaneously.
%Depending on the topology of transformations that preserve the consistency, those challenges are addressed differently, which will be discussed afterwards. %in the subsequent section.
We present already identified challenges in the following. % and discuss them regarding the combination of binary transformations. % as the state-of-the-art for multi-model consistency preservation.

% \compactsubsection{Uniqueness}
% Each consistency relation should only have to be specified once to reduce effort and the risk of contradictory specifications.
% In terms of transformations, this means that there should only be one path in the graph induced by the transformations to propagate information from one model to another.
% Using existing transformation languages, not knowing which relations have already been realized in which transformations can lead to duplication.
% This is especially the case, if one transformation is only the subsumption of others, like in \autoref{fig:binary_combination_example}. %of components and classes defined by
% %mapping \ac{PCM} to \ac{UML} components, \ac{UML} components to \ac{UML} classes, and those classes to Java classes. 
% \todo{Das folgende ist eigentlich schon im vorigen Abschnitt diskutiert worden}
% If the relation \ref{fig:Example:R3} between \ac{ADL} and Java is specified additionally to the transitive relation over \ac{UML}, the transformations derived from it must ensure that after creating an \ac{ADL} component only one Java class is created, that all necessary trace links are created, and that additional constraints, such as the convention to append \enquote{Impl} to the name, are consistently defined.

\begin{description}[leftmargin=\parindent]
\item[Compatibility]
Transformations preserve consistency, but also have to be consistent among themselves, i.e., they have to adhere to the same consistency relations, referred to as \emph{compatibility}.
%Contradictory specifications can, for example, arise from inconsistent realizations of the same consistency relations.
If, in our example in \autoref{fig:prologue:binary_combination_example}, the relation \ref{fig:prologue:binary_combination_example:R3} between \ac{ADL} and Java is realized in addition to the transitive relation $\mbox{\ref{fig:prologue:binary_combination_example:R1}} \concat \mbox{\ref{fig:prologue:binary_combination_example:R2}}$ over \ac{UML}, both relations must contain the same name attribute mapping.
If the \ac{ADL} to Java transformation adds an \enquote{Impl} suffix~\cite{langhammer2017a}, whereas the transformations over \ac{UML} omit that, they are \emph{incompatible}.
This can lead to propagation cycles due to alternating values, and to results depending on the transformation execution order. %depends on the transformation execution order. %, which means that the result depends on the execution order of the transformations. % (e.g. the propagation from \ac{PCM} to Java or from \ac{UML} to Java), the result differs (in the example the name of the class)
%\end{inparaenum}
While \emph{compatibility} concerns problems due to the realization of contradictory consistency relations, %the realization of the same consistency relations by different transformations, 
\emph{interoperability} concerns potentially unexpected behavior although all transformations follow to the same consistency relations.
%\todoErik{Ich finde diese Numerierungen immer ein bißchen affig, wenn man sich später nicht auf die Zahlen bezieht. \enquote{On the one hand \ldots on the other hand} sagt dasselbe aus.}
%A direct implication of inconsistent specifications is the non-uniqueness of the specifications.


\item[Modularity]
The development of different systems requires the usage of different \metamodels to describe them.
%One benefit of binary transformations is that they can be reused in different projects and especially different contexts with different selections of \metamodels.
Therefore, transformations should be modular, so that an arbitrary selection of \metamodels and transformations between them can be used within a concrete project.
If, in our example, transformations are specified transitively across \ac{UML}, it is impossible to use only the \ac{ADL} and Java in a concrete project, omitting the \ac{UML}. 
%This property is especially contradictory to the \emph{uniqueness} property when using transitively executed transformations.

\item[Comprehensibility]
Maintainability of artifacts, including transformations, depends on their comprehensibility.
Comprehensibility can be seen as the number of transformations to consider if a specific consistency relation shall be understood.
Realizing consistency relations in transitive transformations can, for example, reduce comprehensibility, as a developer has to consider a set of transformations to understand a single consistency relation. 
%To ensure %\emph{uniqueness} and 
%transformation consistency %require consistency preservation to be only defined once between two \metamodels, which
%it can be necessary to define transitive transformations %across other \metamodels 
%to avoid redundant specifications.
%This can reduce comprehensibility as a developer has to consider a set of transformations to understand a single consistency relation. %, which is why this is a contradictory challenge to those others.
%\todoErik{Sie kann dadurch aber auch erhöht werden, weil die einzelnen Transformationen weniger komplex werden (divide et impera)} % Ja, das stimmt möglicherweise im Einzelfall. Aber dafür muss ein Transformationsentwickler im Allgemeinen auch potentiell wieder andere Domänen verstehen, was nicht so vorteilhaft ist.

\item[Evolvability]
Whenever new transformations shall be defined, e.g. because a new \metamodel shall be used and transformations for it have to be defined, the effort should be as low as possible.
This concerns the number of transformations to define and how many \metamodels are involved in the transformations for one consistency relation if it is expressed transitively.
\end{description}

%\subsection{Trade-off Solutions}

\begin{figure}[b]
    \centering
    \begin{minipage}[b]{0.4\columnwidth}
        \centering
        \input{figures/properties/topologies_full.tex}
        \subcaption{Dense graph}
        \label{fig:properties:topologies:full}
    \end{minipage}
    \hspace{2em}
    \begin{minipage}[b]{0.4\columnwidth}
        \centering
        \input{figures/properties/topologies_tree.tex}
        \vspace{1em}
        \subcaption{Tree}
        \label{fig:properties:topologies:tree}
    \end{minipage}
    \caption{Extreme examples for transformation topologies}
    \label{fig:properties:topologies}
\end{figure}

%As discussed during the presentation of the challenges in the previous section, 
We will focus on compatibility and modularity, as they are crucial for the applicability of transformations. %, whereas comprehensibility and development effort can be seen as usability problems. %, so we focus on those former two challenges.
Defining binary transformations %to express consistency preservation 
for a set of \metamodels leads to a trade-off solution regarding those challenges, as they cannot be solved simultaneously.
The intuitive way to define transformations %preservation of consistency relations is achieved by expressing each relation in a transformation.
is to express each relation between two \metamodels in one transformation, which leads to a dense graph of transformations. % between the \metamodels.
In the extreme case, if all pairs of \metamodels have non-empty consistency relations, the graph is complete, as shown in \autoref{fig:properties:topologies:full}.
In that case, modularity is high because each \metamodel can be excluded without any drawback, %, comprehensibility is rather high as each relation is expressed directly between the involved \metamodels, %(although it may be additionally expressed transitively), 
but relations are likely to be incompatible, as, in the worst case, each relation is specified over $(n-1)!$ transformation paths if $n$ \metamodels are involved.
While comprehensibility is high, as each relation is explicitly expressed, adding a \metamodel requires to define up to $n-1$ transformations, implying high evolution effort.
%Finally, this also implies a high development effort, as adding a \metamodel requires to define transformations to all existing ones.

Another extreme case is to have each consistency relation only defined over a single path in the transformations graph, which results in a tree of transformations, as shown in \autoref{fig:properties:topologies:tree}.
In that case, compatibility of transformations is inherently given, as each relation is only defined once, but modularity is reduced, as only \metamodels being leaves can be omitted.
Comprehensibility is low, as each relation may be defined in a path of up to $n-1$ transformations, but evolvability is rather good, as each relation must only be defined once. %, but in a way such that the tree structure is maintained. %requires a critical investigation about how to express the consistency relations to maintain the tree structure.
We summarize that impact of the topology in \autoref{tab:properties:topology_impact}.

%In the following sections, we discuss our proposed solutions on how to deal with those trade-offs.
\begin{table}
    \centering
    \begin{tabular} {L{8em} C{6em} C{5em}}
        \toprule
        \textbf{Challenge} & \textbf{Dense Graph} & \textbf{Tree}\\
        \midrule
    %    Uniqueness & - - & ++\\
        Compatibility & - & ++\\
        Modularity & ++ & -\\
        Comprehensibility & + & -\\
        Evolvability & - & +\\
        \bottomrule
    \end{tabular}
    \caption{Challenge fulfillment by transformation topology}
    \label{tab:properties:topology_impact}
    \vspace{-1.5em}
\end{table}

A tree topology has the drawback that it is not always applicable.
It requires that the transformation developers
%\todoErik{whom?}
find a subset of all consistency relations between the \metamodels that induces a tree and whose transitive closure contains all other relations.
%This is necessary, because the transitive relations in the tree must also cover the direct relations between the \metamodels that are omitted.
In general, such a subset cannot be found.
Of three \metamodels, there must always be one containing the overlapping information of the two others, as only then the transitive closure of two consistency relations subsumes the third. 
In the example in \autoref{fig:properties:tree_generation}, it is necessary that two relations are contained in the transitive closure of the others to get a tree that covers all relations.
%An example for such a relation is given in the graphic.

\begin{figure}[b]
    \centering
    \input{figures/properties/tree_generation.tex}
    \caption{Reducing a consistency relation graph to a tree}
    \label{fig:properties:tree_generation}
\end{figure}

In practice, the used topology will potentially be a mixture between those extremes.
The natural way to foster the independent development of transformations %to specify consistency preservation 
would be to define one transformation for each consistency relation, resulting in a dense graph with a high potential for incompatible transformations.
To deal with that, mechanisms that analyze compatibility between transformations could be researched.
Nevertheless, high expressiveness of transformation languages allows only conservative approximations. % without reducing the expressiveness of transformation languages, this easily leads to problems similar to those in static code analysis, which rely on approximations, due to the Halting problem.
In our thesis, we will therefore investigate approaches that result in tree-like specifications that directly imply compatibility between the transformations, but with increased \emph{applicability} and \emph{modularity}. %of  try to achieve a tree of transformations, as this directly implies consistency between the transformations, but with increased modularity and with applicability.


% In our thesis, we will investigate which kinds of topologies are applicable in different case studies and how the underlying consistency relations influence that.
% We will analyze how this can be used to combine the topology extremes reasonably by decomposing the consistency relations of a complete system to solve certain challenges for individual system parts.
% We then plan to develop methods to deal with the identified challenges.
% First, we want to support the 

% In our thesis, we will analyze several case studies regarding the occurring consistency relations and their impact on the challenges when expressing them in binary transformations.
% We will derive guidelines from that on how to define transformations in which context, depending on the kinds of consistency relations existing between the \metamodels, e.g. regarding the composability of fully connected transformation graphs in a higher-level tree.


%Different dimension: Restriction of relations that can be defined with one topology


% Problems with or drawbacks of transitive approaches:
% \begin{enumerate}
% %% Redundanz: Möglicherweise unnötig, mehr Aufwand als nötig
%     \item \textbf{Duplication}: Overlapping consistency relations may be specified between different pairs of \metamodels inducing an undirected cycle, which means that the same consistency relation is realized by different consistency preservation specifications. 
%     Simple case regarding the running example: Assume a consistency preservation specifications between \ac{PCM} and \ac{UML}, as well as between \ac{UML} and Java defining the mapping between classes and components having equals name. Additionally, for example to use the \ac{PCM} and Java without \ac{UML} or due to missing knowledge about existing relations, this relation is implemented between \ac{PCM} and Java as well. In this case, the creation of a \ac{PCM} component is propagated to the Java code due to the consistency preservation specification across \ac{UML} and also due to the direct specification. The engine executing the change propagation has to ensure that the Java class is not created twice and that potentially necessary trace links have to be created between all models correctly.
% %% Wenn redundant, dann muss man es noch richtig machen
%     \item \textbf{Specification Consistency}: Due to the above mentioned fact that consistency relations may specified in duplicate, it is also necessary that those specifications are consistent. Regarding the simple, above mentioned example, it may be possible that the consistency preservation specification between \ac{PCM} and Java defines the mapping between class and component differently with an additional suffix ``Impl'' attached to the Java class name, as proposed by \citeauthor{langhammer2017a}. In that case, the consistency preservation specifications are not consistent. This can lead to two further problems, depending on the implementation:
%     \begin{enumerate}
%         \item Propagation cycles: Due to alternating values, the propagation of a modification by consistency preservation specifications does never end
%         \item Confluence race conditions: Depending on which propagation is executed first (e.g. the propagation from \ac{PCM} to Java or from \ac{UML} to Java), the result differs (in the example the name of the class)
%     \end{enumerate}
% %% Manchmal verliert man den zusätzlichen Aufwand sogar und ich kann meinen eigenen Aufwand nicht wiederverwenden 
% %%%% Auch aus Sprach-Evolutionssicht gut
%     \item \textbf{Modularity}: To avoid inconsistencies between the specification and confluence problems, each consistency relation should ideally be only realized by one consistency preservation specification. This has the drawback that all \modelinglanguages have to be used in a project that wants to use the consistency preservation mechanisms. Leaving out single \modelinglanguages would result in missing models across which consistency is preserved.
% %% und wenn jemand anders es macht kann ich es vielleicht noch nicht einmal wiederverwenden
%     \item \textbf{Comprehensibility}: Since the avoidance of inconsistencies and confluence problems requires consistency to be defined not directly between two \metamodels but transitively across several ones, it is hard to understand the consistency relations realized by the consistency preservation specifications. All defined specifications have to be understood by the specification developer.
% \end{enumerate}



% \subsection{Increasing Transformation Consistency}

% The intuitive definition of consistency preservation is to define transformations for all existing consistency relations.
% While the resulting topology of transformations leads to high modularity, this induces highly interdependent specifications as several transformations are covered by the transitive closure of others, which easily leads to inconsistencies between those specifications.

% In our thesis, we will therefore investigate, how far contradictions in transformations can be automatically detected.
% In the best case, static checks of the transformation specifications can be used to detect inconsistencies.
% Otherwise, at least during runtime, inconsistencies should be detected.

% We will analyze which kind of consistency guarantees can be made between the transformations based on the provided operators.
% This will potentially result in a subset of supported operators for which guarantees can be given or analyzed, whereas others have to be excluded.





\section{Topologies of Transformation Networks}
Map properties to topologies, also from doc sym and more precisely